{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Common data processing\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain.chains import RetrievalQAWithSourcesChain\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from environment\n",
    "load_dotenv('.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# Note the code below is unique to this course environment, and not a \n",
    "# standard part of Neo4j's integration with OpenAI. Remove if running \n",
    "# in your own environment.\n",
    "# OPENAI_ENDPOINT = os.getenv('OPENAI_BASE_URL') + '/embeddings'\n",
    "\n",
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'form_10k_chunks'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'textEmbedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file_name = \"./data/form10k/0000950170-23-027948.json\"\n",
    "first_file_as_object = json.load(open(first_file_name))\n",
    "type(first_file_as_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item1 <class 'str'>\n",
      "item1a <class 'str'>\n",
      "item7 <class 'str'>\n",
      "item7a <class 'str'>\n",
      "cik <class 'str'>\n",
      "cusip6 <class 'str'>\n",
      "cusip <class 'list'>\n",
      "names <class 'list'>\n",
      "source <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for k,v in first_file_as_object.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item1_text = first_file_as_object['item1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud envir'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1_text[0:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item1_text_chunks = text_splitter.split_text(item1_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(item1_text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item1_text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1_text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_form10k_data_from_file(file):\n",
    "    chunks_with_metadata = [] # use this to accumlate chunk records\n",
    "    file_as_object = json.load(open(file)) # open the json file\n",
    "    for item in ['item1','item1a','item7','item7a']: # pull these keys from the json\n",
    "        print(f'Processing {item} from {file}') \n",
    "        item_text = file_as_object[item] # grab the text of the item\n",
    "        item_text_chunks = text_splitter.split_text(item_text) # split the text into chunks\n",
    "        chunk_seq_id = 0\n",
    "        for chunk in item_text_chunks[:20]: # only take the first 20 chunks\n",
    "            form_id = file[file.rindex('/') + 1:file.rindex('.')] # extract form id from file name\n",
    "            # finally, construct a record with metadata and the chunk text\n",
    "            chunks_with_metadata.append({\n",
    "                'text': chunk, \n",
    "                # metadata from looping...\n",
    "                'f10kItem': item,\n",
    "                'chunkSeqId': chunk_seq_id,\n",
    "                # constructed metadata...\n",
    "                'formId': f'{form_id}', # pulled from the filename\n",
    "                'chunkId': f'{form_id}-{item}-chunk{chunk_seq_id:04d}',\n",
    "                # metadata from file...\n",
    "                'names': file_as_object['names'],\n",
    "                'cik': file_as_object['cik'],\n",
    "                'cusip6': file_as_object['cusip6'],\n",
    "                'source': file_as_object['source'],\n",
    "            })\n",
    "            chunk_seq_id += 1\n",
    "        print(f'\\tSplit into {chunk_seq_id} chunks')\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item1 from ./data/form10k/0000950170-23-027948.json\n",
      "\tSplit into 20 chunks\n",
      "Processing item1a from ./data/form10k/0000950170-23-027948.json\n",
      "\tSplit into 1 chunks\n",
      "Processing item7 from ./data/form10k/0000950170-23-027948.json\n",
      "\tSplit into 1 chunks\n",
      "Processing item7a from ./data/form10k/0000950170-23-027948.json\n",
      "\tSplit into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "first_file_chunks = split_form10k_data_from_file(first_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.',\n",
       " 'f10kItem': 'item1',\n",
       " 'chunkSeqId': 0,\n",
       " 'formId': '0000950170-23-027948',\n",
       " 'chunkId': '0000950170-23-027948-item1-chunk0000',\n",
       " 'names': ['Netapp Inc', 'NETAPP INC'],\n",
       " 'cik': '1002047',\n",
       " 'cusip6': '64110D',\n",
       " 'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 23)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first_file_chunks), len(first_file_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 0,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0000',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': \"•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.\\n\\n\\n•\\nCyber resilience: NetApp unifies monitoring, data protection, security, governance, and compliance for total cyber resilience - with consistency and automation across environments. \\n\\n\\n•\\nContinuous operations: NetApp uses AI-driven automation for continuous optimization to service applications and store stateless and stateful applications at the lowest possible costs.\\n\\n\\n•\\nSustainability: NetApp has industry-leading tools to audit consumption, locate waste, and set guardrails to stop overprovisioning.\\n\\n\\nProduct, Solutions and Services Portfolio\\n \\n\\n\\nNetApp's portfolio of cloud services and storage infrastructure is powered by intelligent data management software. Our operations are organized into two segments: Hybrid Cloud and Public Cloud.\\n\\n\\n \\n\\n\\nHybrid Cloud\\n\\n\\nHybrid Cloud \\noffers a portfolio of storage management and infrastructure solutions that help customers recast their traditional data centers into modern data centers with the power of the cloud. Our hybrid cloud portfolio is designed to operate with public clouds to unlock the potential of hybrid, multi-cloud operations. We offer a broad portfolio of cloud-connected all-flash, hybrid-flash, and object storage systems, powered by intelligent data management software. Hybrid Cloud is composed of software, hardware, and related support, as well as professional and other services.\\n\\n\\nIntelligent data management software\",\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 1,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0001',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': \"Intelligent data management software\\n\\n\\nNetApp ONTAP\\n software is our foundational technology that underpins NetApp's critical storage solutions in the data center and the cloud. ONTAP includes various data management and protection features and capabilities, including automatic ransomware protection against cyber-attacks, built-in data transport features, and storage efficiency capabilities. ONTAP provides the flexibility to design and deploy a storage environment across the broadest range of architectures – from on-premises, hybrid, public, and private clouds. It can be used in NAS, SAN, object environments, and software-defined storage (SDS) situations.\\n\\n\\nData integrity and safety are at the heart of any company’s data center. With NetApp’s extensive software tools and utilities, customers can realize their business continuity goals with time, costs, and personnel savings. With \\nNetApp Snapshot\\n, customers can create and manage point-in-time file system copies with no performance impact and minimal storage consumption. This is important for continuous data protection of information in read-only, static, and immutable form. \\nNetApp SnapCenter Backup Management\\n \\ns\\noftware\\n \\nis designed to deliver high-performance backup and recovery for database and application workloads hosted on ONTAP storage. \\nNetApp SnapMirror Data Replication\\n software can replicate data at high speeds across environments. SnapMirror delivers\\n \\n\\n\\n6\\n\\n\\n\\n\\n\\xa0\\n\\n\\nrobust data management capabilities for virtualization, protecting critical data while providing the flexibility to move data between locations and storage tiers, including cloud service providers. \\nNetApp SnapLock Data Compliance \\nsoftware delivers high-performance disk-based data permanence for HDD and SSD deployments.\",\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 2,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0002',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'NetApp Astra (Astra)\\n is a fully managed application-aware data management service built for emerging Kubernetes workloads container infrastructures. Astra allows organizations to protect, recover, and move applications deployed on Kubernetes with no software to download, install, manage, or upgrade.\\n \\n\\n\\nStorage infrastructure\\n \\n\\n\\nNetApp All-Flash FAS (AFF A-Series)\\n is a scale-out platform built for virtualized environments, combining low-latency performance via flash memory (also known as a solid-state storage disk) with best-in-class data management, built-in efficiencies, integrated data protection, multiprotocol support, and nondisruptive operations; cloud and on-premises. AFF A-Series, powered by ONTAP, allows customers to connect to clouds for more data services, data tiering, caching, and disaster recovery. The AFF A-Series has a portfolio of products designed for multiple markets and price/performance considerations, from smaller channel commercial market offerings to large-scale, global enterprises.\\n\\n\\nNetApp QLC-Flash FAS (AFF C-Series) \\nis NetApp’s newest family of storage infrastructure solutions. AFF C-Series arrays are sustainable, scalable, and secure solutions for Tier 1 and Tier 2 applications. AFF C-series provides customers capacity flash performance and affordability, so that customers do not need to make compromises. The AFF C-Series is ideal for transitioning from hybrid/HDD to all-flash storage; running non-latency sensitive VMware database applications and file environments; and providing a solution for secondary storage targets for disaster recovery, backup, and tiering.\\n \\n\\n\\nNetApp Fabric Attached Storage (FAS)\\n series\\n \\nare high-volume, high-capacity data storage devices powered by NetApp ONTAP. NetApp FAS Storage Arrays provide customers with a balance of performance and capacity running either disk drives or hybrid-flash configurations. FAS systems are suitable for secondary storage targets for disaster recovery, backup, and tiering.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 3,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0003',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'NetApp E/EF series\\n is built for dedicated, high-bandwidth applications that need simple, fast SAN storage with enterprise-grade reliability. The E-Series is available as a hybrid-flash platform, while the EF-Series is all-flash. On the SANtricity storage operating system, the E/EF-Series storage appliances are designed for performance-sensitive workloads like real-time analytics, high performance computing, and databases.\\n \\n\\n\\nNetApp StorageGRID\\n is a software-defined object storage solution for large archives, media repositories, and web data stores. Using the industry-standard object APIs like the Amazon Simple Storage Service (S3), the StorageGRID solution, running on the ElementOS data management storage operating system, is provided as a NetApp-branded storage solution and as a software-defined solution on third-party hardware.\\n \\n\\n\\n\\xa0\\n\\n\\nPublic Cloud\\n\\n\\nPublic Cloud\\n offers a portfolio of products delivered primarily as-a-service, including related support. This portfolio includes cloud storage and data services and cloud operations services. Our enterprise-class solutions and services enable customers to control and manage storage in the cloud, consume high-performance storage services for primary workloads, and optimize cloud environments for cost and efficiency. These solutions and services are generally available on the leading public clouds, including Amazon AWS, Microsoft Azure, and Google Cloud Platform.\\n \\n\\n\\nCloud storage, data services, and software',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 4,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0004',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'Cloud storage, data services, and software\\n\\n\\nThe NetApp Cloud Volumes Platform is an integrated collection of cloud storage infrastructure and data services. The platform is anchored by \\nNetApp Cloud Volumes ONTAP\\n, a cloud-based software for customers who wish to manage their own cloud storage infrastructure. It is based on the same ONTAP data management software that underpins our storage infrastructure offerings. Fully managed cloud storage offerings are available natively on Microsoft Azure as \\nAzure NetApp Files\\n, on AWS as \\nAmazon FSx for NetApp ONTAP\\n, and on Google Cloud as \\nNetApp Cloud Volumes Service for Google Cloud.\\n \\n\\n\\nManageability\\n\\n\\nAt the heart of our public cloud storage and data service offerings is \\nNetApp\\n \\nBlueXP\\n. BlueXP is a unified control plane that enables customers to manage their entire data landscape through one single, SaaS-delivered point of control. NetApp BlueXP combines storage and data services via its unified control plane to change how hybrid, multicloud environments are managed, optimized, and controlled. An intuitive interface and powerful automation help decrease resource waste, complexity, and the risk of managing diverse environments. It brings customers operational simplicity in a complex world. Within BlueXP are standard and optional capabilities (services) which allow customers to control their data and operations.\\n\\n\\n7',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 5,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0005',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': '7\\n\\n\\n\\n\\n\\xa0\\n\\n\\nWith \\nBlueXP Sync\\n service, customers can migrate data to the cloud securely and efficiently. Customers can choose where to deploy primary workloads without re-architecting applications or databases. Customers also get a comprehensive, industry-leading portfolio of storage efficiency capabilities. Inline data compression, deduplication, compaction, and cloud tiering (\\nBlueXP Tiering \\nservice) work together to reduce storage costs and maximize data storage. \\nNetApp Backup \\nservice delivers seamless and cost-effective backup and restore capabilities for protecting and archiving cloud and on-premises data managed by ONTAP. \\nBlueXP\\n \\nCompliance \\nservice provides data discovery, mapping, and classification driven by artificial intelligence algorithms with automated controls and reporting for data privacy regulations such as the General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), and more. Lastly, the \\nBlueXP Cache \\nservice delivers fast and secure access to data for users by caching active data sets to distributed offices globally.\\n \\n\\n\\nCloud operations services\\n\\n\\nNetApp Cloud Insights\\n is an infrastructure monitoring tool that gives organizations visibility into their entire infrastructure. It can monitor, troubleshoot, and optimize costs across all resources, including public clouds and private data centers. Working in conjunction with the BlueXP manageability and control plane services, customers can have deep insights into their data operations.\\n\\n\\nOur \\nSpot by NetApp \\nsuite of products delivers a platform for cloud operations, enabling customers to deploy and operate cloud applications reliably and securely in their choice of the cloud while reducing costs and complexity. Combining machine learning, predictive analytics, and cloud automation, the Spot platform continuously optimizes cloud infrastructure and operations to deliver scalable, reliable, and secure application infrastructure.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 6,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0006',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': \"Another cloud operations service is \\nInstaclustr\\n, our platform that provides fully managed open-source databases, pipelines, and workflow applications delivered as a service. Instaclustr helps organizations deliver cloud-native applications at scale by operating and supporting their data infrastructure through its SaaS platform for those designing and building around open-source technologies.\\n \\n\\n\\nProfessional and Support Services\\n\\n\\nNetApp and our certified services partners offer a comprehensive portfolio of assessment, design, implementation, migration, and proactive support services to help customers optimize the performance and efficiency of their on-premises and hybrid multicloud storage environments. Our portfolio of offerings include strategic consulting, professional, managed, and support services.\\n \\n\\n\\n•\\nNetApp strategic consulting services provide executive-level, high-touch consulting engagements to help organizations facilitate the alignment of their business and technology goals. Our proven expertise can help organizations define long-term data fabric strategies and operations models to drive IT initiatives for digital transformation.\\n \\n\\n\\n•\\nNetApp Professional Services provide the expertise to mitigate risk and streamline the design, implementation, migration, and integration of NetApp hybrid cloud solutions to realize the business benefits of new technology investments faster. Highly skilled services experts help enable secure, optimized environments that deliver the consistent, high-quality outcomes customers expect from the start.\\n \\n\\n\\n•\\nNetApp Managed Services optimize performance and efficiency in hybrid cloud and on-premises environments. Our NetApp experts use proven methodology and best practices to monitor, administer, operate, and optimize customer environments so their organization's IT staff is free to focus on initiatives to move the business forward.\",\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 7,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0007',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': '•\\nNetApp Keystone is our pay-as-you-grow, storage-as-a-service (STaaS) offering that delivers a seamless hybrid cloud experience for those preferring operating expense consumption models to upfront capital expense or leasing. With a unified management console and monthly bill for both on-premises and cloud data storage services, Keystone lets organizations provision and monitor, and even move storage spend across their hybrid cloud environment for financial and operational flexibility. \\n\\n\\n•\\nNetApp Global Support supplies systems, processes, and people wherever needed to provide continuous operation in complex and critical environments, with an emphasis on proactive and preemptive technology support for operational continuity across the NetApp hybrid cloud. Personalized support options provide actionable intelligence to resolve problems faster, reduce downtime, and optimize performance of the entire NetApp ecosystem.\\n\\n\\nSales, Principal Markets, and Distribution Channels\\n\\n\\nWe market and sell our products and services in numerous countries throughout the world.  Our sales efforts are organized around the evolving needs of our current and targeted customers, and our marketing initiatives reflect this focus. NetApp uses a multichannel distribution strategy. We sell our products, solutions and services to end-user business customers and service providers through a direct sales force and an ecosystem of partners, including the leading cloud providers. Our marketing is focused on building our brand reputation, creating market awareness, communicating customer advantages and generating demand for our sales force and channel partners.\\n\\n\\n8',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 8,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0008',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': '8\\n\\n\\n\\n\\n\\xa0\\n\\n\\nOur diversified customer base spans industry segments and vertical markets such as energy, financial services, government, technology, internet, life sciences, healthcare services, manufacturing, media, entertainment, animation, video postproduction and telecommunications. NetApp focuses primarily on the enterprise storage and data management, cloud storage and cloud operations markets. We design our products to meet the evolving requirements of a hybrid, multicloud world, driven by digital transformation and cloud initiatives.\\n\\n\\nOur partnerships with the industry’s leading cloud, infrastructure, consulting, application, and reseller partners are created with one goal in mind: the success of our customers. Global enterprises, local businesses, and government installations look to NetApp and our ecosystem of partners to help maximize the business value of their IT and cloud investments.\\n\\n\\nWe work with a wide range of partners for our customers, including technology partners, value-added resellers, system integrators, OEMs, service providers and distributors. During fiscal 2023, sales through our indirect channels represented 78% of our net revenues. Our global partner ecosystem is critical to NetApp’s growth and success. We are continually strengthening existing partnerships and investing in new ones to ensure we are meeting the evolving needs of our customers.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 9,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0009',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'As of April 28, 2023, our worldwide sales and marketing functions consisted of approximately 5,700 managers, sales representatives and technical support personnel. We have offices in approximately 25 countries. Sales to customers Arrow Electronics, Inc. and Tech Data Corporation accounted for 24% and 21% of our net revenues, respectively, in fiscal 2023. Information about sales to and accounts receivables from our major customers, segment disclosures, foreign operations and net sales attributable to our geographic regions is included in Note 15 – Segment, Geographic, and Significant Customer Information of the Notes to Consolidated Financial Statements.\\n\\n\\nSeasonality\\n\\n\\nWe have historically experienced a sequential decline in revenues in the first quarter of our fiscal year, as the sales organization spends time developing new business after higher close rates in the fourth quarter, and because sales to European customers are typically weaker during the summer months. We derive a substantial amount of our revenue in any given quarter from customer orders booked in the same quarter. Customer orders and revenues typically follow intra-quarter seasonality patterns weighted toward the back end of the quarter. If recurring services and cloud revenue continue to increase as a percentage of our total revenues, historical seasonal patterns may become less pronounced.\\n\\n\\nBacklog',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 10,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0010',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'Backlog\\n\\n\\nWe manufacture products based on a combination of specific order requirements and forecasts of our customers’ demand. Orders are generally placed by customers on an as-needed basis. A substantial portion of our products is sold on the basis of standard purchase orders that are cancelable prior to shipment without penalty. In certain circumstances, purchase orders are subject to change with respect to quantity of product or timing of delivery resulting from changes in customer requirements. Our business is characterized by seasonal and intra-quarter variability in demand, as well as short lead times and product delivery schedules. Accordingly, backlog at any given time may not be a meaningful indicator of future revenue.\\n\\n\\nManufacturing and Supply Chain',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 11,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0011',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'Manufacturing and Supply Chain\\n\\n\\nWe have outsourced manufacturing operations to third parties located in Fremont, California; San Jose, California; San Antonio, Texas; Guadalajara, Mexico; Schiphol Airport, The Netherlands; Tiszaujvaros, Hungary; Wuxi, China; Taoyuan City, Taiwan; and Singapore. These operations include materials procurement, commodity management, component engineering, test engineering, manufacturing engineering, product assembly, product assurance, quality control, final test, and global logistics. We rely on a limited number of suppliers for materials, as well as several key subcontractors for the production of certain subassemblies and finished systems. We strive to have multiple suppliers qualified to provide critical components where possible and have our products manufactured in a number of locations to mitigate our supply chain risk. Our strategy has been to develop close relationships with our suppliers, maximizing the exchange of critical information and facilitating the implementation of joint quality programs. We use contract manufacturers for the production of major subassemblies and final system configuration. This manufacturing strategy minimizes capital investments and overhead expenditures while creating flexibility for rapid expansion.\\n\\n\\nWe are certified to the International Organization for Standardization (ISO) 9001:2015 and ISO 14001:2015 certification standards. We have been Tier 2 certified under the U.S. Customs and Border Protection’s (CBP) Customs Trade Partnership Against Terrorism (CTPAT) program since January 2015.\\n\\n\\nResearch and Development',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 12,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0012',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'Research and Development\\n\\n\\nOur research and development team delivers innovation to help customers create an evolved cloud experience. Our R&D structure allows us to align and accelerate the execution of our strategies and roadmaps across product groups. We leverage our talent and shared IP for cloud- and hybrid-cloud solutions to remain agile to changing market conditions. Our R&D priorities are to help\\n \\n\\n\\n9\\n\\n\\n\\n\\n\\xa0\\n\\n\\ncustomers break down silos to simplify management, create consistency, and deliver observability across on premises and multiple cloud environments. We design our products and services from the ground up with cloud connectivity in mind, including tiering, disaster recovery, replication, bursting, and migration.\\n\\n\\nWe conduct research and development activities in various locations throughout the world. Total research and development expenses were $956 million in fiscal 2023, and $881 million in each of fiscal 2022 and fiscal 2021. These costs consist primarily of personnel and related costs incurred to conduct product development activities. Although we develop many of our products internally, we also acquire technology through business combinations or through licensing from third parties when appropriate. We believe that technical leadership is essential to our success, and we expect to continue to commit substantial resources to research and development.\\n\\n\\nCompetition\\n\\n\\nWe operate in an industry in which there are rapid technological advances in hardware, software, and related services offerings. Cloud, digital transformation, and artificial intelligence initiatives are driving changes in customer and solution requirements.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 13,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0013',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'We compete with many companies in the markets we serve. Our hybrid cloud solutions primarily compete with legacy IT and storage vendors. Some offer a broad spectrum of products, solutions and services and others offer a more limited set of storage- and data-management products, solutions or services. Additionally, public cloud providers offer customers storage as an operating expense which competes with more traditional storage offerings that customers acquire through capital expenditures. We both partner with and compete against cloud providers with our public cloud software and services. We rarely see legacy vendors competing in the cloud.\\n\\n\\nWe compete with many companies in the cloud operations marketplace, including new companies (startups) and larger software companies who target developers, operations engineering (DevOps) and financial engineering (FinOps). Some companies have single point solutions that compete with one of our services and others are building platforms. Additionally public cloud providers offer similar services on their own cloud.\\n\\n\\nWe face ongoing product and price competition in all areas of our business, including from both branded- and generic-product competitors.\\n \\n\\n\\nOur current and potential competitors may establish cooperative relationships among themselves or with third parties, including some of our partners. It is possible that new competitors or alliances among competitors might emerge and further increase competitive pressures.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 14,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0014',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'We consider our software innovation, cloud integration, and technology partnerships key to our competitive differentiation. We believe our competitive advantage also includes the nature of the relationships we form with our customers and partners worldwide. We strive to deliver an outstanding experience in every interaction we have with our customers and partners through our product, service, and support offerings, which enables us to provide our customers a full range of expertise before, during and after their purchases.\\n\\n\\nProprietary Rights\\n\\n\\nWe generally rely on patent, copyright, trademark, trade secret and contract laws to establish and maintain our proprietary rights in our technology, products and services. While our intellectual property rights are important to our success, we believe that our business is not materially dependent on any particular patent, trademark, copyright, license or other individual intellectual property right. We have been granted, or own by assignment\\n,\\n well over two thousand U.S. patents, hundreds of pending U.S. patent applications, and many corresponding patents and patent applications in other countries. From time to time, we may make certain intellectual property available under an open source license. Our primary trademarks are NetApp and the NetApp design logo, which are registered trademarks in the U.S. and in many other countries.  In addition, we have trademarks and trademark registrations in the U.S. and other countries covering our various product or service names.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 15,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0015',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'We generally enter into confidentiality agreements with our employees, resellers, distributors, customers, and suppliers. In addition, through various licensing arrangements, we receive certain rights to the intellectual property of others. We expect to maintain current licensing arrangements and to secure additional licensing arrangements in the future, as needed and to the extent available on reasonable terms and conditions, to support continued development and sales of our products and services. Some of these licensing arrangements require or may require royalty payments and other licensing fees. The amount of these payments and fees may depend on various factors, including but not limited to the structure of royalty payments; offsetting considerations, if any; and the degree of use of the licensed technology.\\n\\n\\nThe industry in which we compete is characterized by rapidly changing technology, a large number of patents, and frequent claims and related litigation regarding intellectual property rights, and we may be exposed to various risks related to such claims or legal\\n \\n\\n\\n10\\n\\n\\n\\n\\n\\xa0\\n\\n\\nproceedings. If we are unable to protect our intellectual property, we may be subject to increased competition that could materially and adversely affect our business operations, financial condition, results of operations and/or cash flows.\\n\\n\\nEnvironmental Disclosure\\n\\n\\nWe are committed to the success of our customers and partners, to delivering value to our stockholders, and to positively affecting the communities where our employees work and live. We firmly believe that we can accomplish these objectives concurrently with our commitment to sound environmental management. We are committed to the reduction of greenhouse gas emissions; efficient use of natural resources; and minimizing, relative to the growth of the Company, the environmental impacts from our operations, products, and services, as well as complying with laws and regulations related to these areas.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 16,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0016',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'We voluntarily measure, monitor, and publicly report our scope 1, scope 2, and scope 3 (partial) greenhouse gas emissions and water impacts to CDP, a global standardized mechanism by which companies report their greenhouse gas emissions and water impacts to customers and institutional investors. We continuously seek to optimize the energy efficiency of our buildings, labs, and data centers; and we have increased our use of renewable energy, especially at our facilities in Bangalore, India and Wichita, Kansas, both of which are powered almost exclusively by renewable energy.\\n\\n\\nAt both the global and regional/state levels, various laws and regulations have been implemented or are under consideration to mitigate or report on the effects of climate change. Environmental laws are complex, change frequently, and have tended to become more stringent over time. However, it is often difficult to anticipate future regulations pertaining to environmental matters and to estimate their impacts on our operations. Based on current information, we believe that our primary risk related to climate change is the risk of increased energy costs. We are not currently subject to a cap-and-trade system or any other mitigation measures that could be material to our operations, nor are we aware of any such measures that will impact us in the near future. Additionally, we have implemented disaster recovery and business resiliency measures to mitigate the physical risks our facilities, business, and supply chain might face as a consequence of severe weather/climate-related phenomena such as earthquakes, floods, droughts, and other such natural occurrences.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 17,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0017',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'We are subject to international, federal, state, and local regulations regarding workplace safety and protection of the environment. Various international, federal, state, and local provisions regulate the use and discharge of certain hazardous materials used in the manufacture of our products. Failure to comply with environmental regulations in the future could cause us to incur substantial costs, subject us to business interruptions or cause customers to cease purchasing from us. We strive to comply with all applicable environmental laws. All of our products meet the applicable requirements of the Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH); Energy Related Products (ErP); Restriction of Hazardous Substances (RoHS); and China RoHS directives. We have a product take-back program and an e-waste scheme to comply with the EU directive on Waste Electrical and Electronic Equipment (WEEE), and Extended Producer Responsibility (EPR) regulations in India.\\n\\n\\nWe have maintained an environmental management system since December 2004 that provides the framework for setting, monitoring, and continuously improving our environmental goals and objectives. As part of ISO 14001 requirements, we set local environmental performance goals, such as reducing energy use per square foot and minimizing waste generated on site, that are aligned with our overall corporate strategy. We also conduct periodic reviews and are subject to third-party audits of our operations, and we monitor environmental legislation and requirements to help make sure we are taking necessary measures to remain in compliance with applicable laws, not only in our operations but also for our products.\\n\\n\\nHuman Capital',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 18,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0018',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': 'Human Capital\\n\\n\\nWe take pride in, and believe our success depends on, attracting and retaining leading talent in the industry based on a culture-fit approach. From our inception, NetApp has worked to build a model company and has embraced a culture of openness and trust. Our employees are supported and encouraged to be innovative, and we communicate openly and transparently so that employees can focus on critical and impactful work that ties directly to our business strategy. We continue to invest in our global workforce to support diversity and inclusion and to support our employees’ well-being and development.\\n \\n\\n\\nDiversity, Inclusion and Belonging\\n\\n\\nWe believe diversity, inclusion and belonging leads to more innovation, better access to talent and improved business outcomes. Our strategies are intended to increase the demographic and cognitive diversity of our employee population, promote a culture of inclusion and to leverage such diversity to achieve business results. For more information about our commitment to diversity, inclusion and belonging, go to the “Diversity Inclusion and Belonging” section of our website.\\n\\n\\nBenefits, Wellbeing and Engagement\\n\\n\\nOur healthcare options offer competitive, comprehensive coverage for our employees and their families, including:\\n \\n\\n\\n11\\n\\n\\n\\n\\n\\xa0\\n\\n\\n•\\nNational medical plans,\\n\\n\\n•\\nRegional medical plans,\\n\\n\\n•\\nExpert advice from world-renowned doctors through our medical second opinion program,\\n\\n\\n•\\nNational dental plans,\\n\\n\\n•\\nNational vision plans with two levels of coverage to choose from and a\\n\\n\\n•\\nRobust wellness program.\\n\\n\\nInsurance and income protection\\n. We provide life, accidental death and dismemberment and disability insurance programs. For additional peace of mind, we also offer supplemental insurance for our employees and their dependents.',\n",
       "  'f10kItem': 'item1',\n",
       "  'chunkSeqId': 19,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1-chunk0019',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': '>Item 1A\\n\\n\\n\\xa0\\n\\n\\nRisk Factors\\n\\n\\n\\xa0\\n\\n\\n14',\n",
       "  'f10kItem': 'item1a',\n",
       "  'chunkSeqId': 0,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item1a-chunk0000',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': '>Item 7\\n\\n\\n\\xa0\\n\\n\\nManagement’s Discussion and Analysis of Financial Condition and Results of Operations\\n\\n\\n\\xa0\\n\\n\\n33',\n",
       "  'f10kItem': 'item7',\n",
       "  'chunkSeqId': 0,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item7-chunk0000',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'},\n",
       " {'text': '>Item 7A\\n\\n\\n\\xa0\\n\\n\\nQuantitative and Qualitative Disclosures About Market Risk\\n\\n\\n\\xa0\\n\\n\\n50',\n",
       "  'f10kItem': 'item7a',\n",
       "  'chunkSeqId': 0,\n",
       "  'formId': '0000950170-23-027948',\n",
       "  'chunkId': '0000950170-23-027948-item7a-chunk0000',\n",
       "  'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "  'cik': '1002047',\n",
       "  'cusip6': '64110D',\n",
       "  'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create graph nodes using text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chunk_node_query = \"\"\"\n",
    "MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        mergedChunk.names = $chunkParam.names,\n",
    "        mergedChunk.formId = $chunkParam.formId, \n",
    "        mergedChunk.cik = $chunkParam.cik, \n",
    "        mergedChunk.cusip6 = $chunkParam.cusip6, \n",
    "        mergedChunk.source = $chunkParam.source, \n",
    "        mergedChunk.f10kItem = $chunkParam.f10kItem, \n",
    "        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "        mergedChunk.text = $chunkParam.text\n",
    "RETURN mergedChunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mergedChunk': {'formId': '0000950170-23-027948',\n",
       "   'f10kItem': 'item1',\n",
       "   'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "   'cik': '1002047',\n",
       "   'cusip6': '64110D',\n",
       "   'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm',\n",
       "   'text': '>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.',\n",
       "   'chunkId': '0000950170-23-027948-item1-chunk0000',\n",
       "   'chunkSeqId': 0}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(merge_chunk_node_query, \n",
    "         params={'chunkParam':first_file_chunks[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'name': 'index_343aff4e',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': None,\n",
       "  'readCount': 0},\n",
       " {'id': 1,\n",
       "  'name': 'index_f7700477',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'RELATIONSHIP',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': None,\n",
       "  'readCount': 0},\n",
       " {'id': 2,\n",
       "  'name': 'unique_chunk',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'RANGE',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['chunkId'],\n",
       "  'indexProvider': 'range-1.0',\n",
       "  'owningConstraint': 'unique_chunk',\n",
       "  'lastRead': None,\n",
       "  'readCount': 0}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"SHOW INDEXES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item7a-chunk0000\n",
      "Created 23 nodes\n"
     ]
    }
   ],
   "source": [
    "node_count = 0\n",
    "for chunk in first_file_chunks:\n",
    "    print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkId']}\")\n",
    "    kg.query(merge_chunk_node_query, \n",
    "            params={\n",
    "                'chunkParam': chunk\n",
    "            })\n",
    "    node_count += 1\n",
    "print(f\"Created {node_count} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nodeCount': 23}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "         MATCH (n)\n",
    "         RETURN count(n) as nodeCount\n",
    "         \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://neo4j.com/docs/cypher-manual/current/indexes/semantic-indexes/vector-indexes/\n",
    "\n",
    "vector index : INTEGER between 1 and 4096 inclusively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "{code: Neo.DatabaseError.Statement.ExecutionFailed} {message: 'vector.dimensions' must be between 1 and 4096 inclusively}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43m         CREATE VECTOR INDEX `form_10k_chunks` IF NOT EXISTS\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m          FOR (c:Chunk) ON (c.textEmbedding) \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m          OPTIONS \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m indexConfig: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m            `vector.dimensions`: 8192,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m            `vector.similarity_function`: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m    \u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m         }}\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/langchain_community/graphs/neo4j_graph.py:431\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneo4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Neo4jError\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     data, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/driver.py:971\u001b[0m, in \u001b[0;36mDriver.execute_query\u001b[0;34m(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid routing control value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrouting_\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    969\u001b[0m     )\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_pipelined_begin:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDRIVER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_transformer_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/session.py:581\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransaction_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# if cancellation callback has not been called yet:\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_work/query.py:144\u001b[0m, in \u001b[0;36munit_of_work.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/driver.py:1307\u001b[0m, in \u001b[0;36m_work\u001b[0;34m(tx, query, parameters, transformer)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_work\u001b[39m(\n\u001b[1;32m   1302\u001b[0m     tx: ManagedTransaction,\n\u001b[1;32m   1303\u001b[0m     query: te\u001b[38;5;241m.\u001b[39mLiteralString,\n\u001b[1;32m   1304\u001b[0m     parameters: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny],\n\u001b[1;32m   1305\u001b[0m     transformer: t\u001b[38;5;241m.\u001b[39mCallable[[Result], t\u001b[38;5;241m.\u001b[39mUnion[_T]],\n\u001b[1;32m   1306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m-> 1307\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transformer(res)\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/transaction.py:195\u001b[0m, in \u001b[0;36mTransactionBase.run\u001b[0;34m(self, query, parameters, **kwparameters)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    194\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwparameters)\n\u001b[0;32m--> 195\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tx_ready_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/result.py:175\u001b[0m, in \u001b[0;36mResult._tx_ready_run\u001b[0;34m(self, query, parameters)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tx_ready_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# BEGIN+RUN does not carry any extra on the RUN message.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# BEGIN {extra}\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# RUN \"query\" {parameters} {extra}\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/result.py:231\u001b[0m, in \u001b[0;36mResult._run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pull()\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39msend_all()\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/result.py:425\u001b[0m, in \u001b[0;36mResult._attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhausted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_common.py:184\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_bolt.py:994\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    991\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    992\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    993\u001b[0m )\n\u001b[0;32m--> 994\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_bolt5.py:496\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_common.py:254\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    252\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hydrate_error(metadata)\n",
      "\u001b[0;31mDatabaseError\u001b[0m: {code: Neo.DatabaseError.Statement.ExecutionFailed} {message: 'vector.dimensions' must be between 1 and 4096 inclusively}"
     ]
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "         CREATE VECTOR INDEX `form_10k_chunks` IF NOT EXISTS\n",
    "          FOR (c:Chunk) ON (c.textEmbedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 4096,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'name': 'form_10k_chunks',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'VECTOR',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['textEmbedding'],\n",
       "  'indexProvider': 'vector-2.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': None,\n",
       "  'readCount': 0},\n",
       " {'id': 0,\n",
       "  'name': 'index_343aff4e',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2024, 11, 12, 6, 10, 12, 508000000, tzinfo=<UTC>),\n",
       "  'readCount': 3},\n",
       " {'id': 1,\n",
       "  'name': 'index_f7700477',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'RELATIONSHIP',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': None,\n",
       "  'readCount': 0},\n",
       " {'id': 2,\n",
       "  'name': 'unique_chunk',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'RANGE',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['chunkId'],\n",
       "  'indexProvider': 'range-1.0',\n",
       "  'owningConstraint': 'unique_chunk',\n",
       "  'lastRead': neo4j.time.DateTime(2024, 11, 12, 6, 10, 24, 150000000, tzinfo=<UTC>),\n",
       "  'readCount': 91}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"SHOW INDEXES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')#.to(device)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)#.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.48059621e-01, -2.09049154e-02,  1.80991009e-01,\n",
       "        -1.75338298e-01,  3.16612184e-01, -2.21977547e-01,\n",
       "        -7.08250850e-02,  5.53206325e-01, -3.59156132e-02,\n",
       "         1.43371254e-01,  1.58441916e-01, -4.08272445e-01,\n",
       "        -2.23827049e-01,  7.61420846e-01, -1.25323102e-01,\n",
       "         3.18867475e-01, -4.58233431e-02,  9.09725949e-02,\n",
       "        -3.23135406e-01,  3.13324749e-01,  8.17095563e-02,\n",
       "        -2.57612944e-01,  2.61395931e-01,  6.54649734e-01,\n",
       "         3.40279311e-01, -2.69733965e-01,  4.23296029e-03,\n",
       "        -4.40138653e-02,  6.21559285e-02, -2.52598356e-02,\n",
       "         2.19459370e-01, -3.47875357e-02,  4.10643190e-01,\n",
       "        -5.91001995e-02, -2.79651731e-01,  2.59392291e-01,\n",
       "        -3.41561943e-01, -2.19262112e-02,  9.51049700e-02,\n",
       "         1.26200244e-01, -2.25273728e-01, -5.20625293e-01,\n",
       "        -1.67916074e-01, -3.51750292e-02,  1.10207610e-01,\n",
       "        -2.99208939e-01,  6.88232249e-03,  6.62061274e-01,\n",
       "         7.53848672e-01, -8.51377398e-02, -3.71098131e-01,\n",
       "        -8.94969851e-02,  3.12911332e-01,  8.67219791e-02,\n",
       "         3.08945954e-01,  9.28874671e-01, -3.16767484e-01,\n",
       "        -5.56692898e-01, -7.17401877e-02, -1.06809542e-01,\n",
       "        -2.18904227e-01, -6.81597833e-03, -1.53334215e-01,\n",
       "        -2.85529315e-01,  3.37484837e-01,  1.70261070e-01,\n",
       "        -2.77599897e-02,  2.54107773e-01, -9.93641317e-01,\n",
       "        -8.66931006e-02, -2.59923786e-01, -2.08108172e-01,\n",
       "         3.09717953e-01,  2.75673449e-01, -1.65710643e-01,\n",
       "         2.94001490e-01, -5.06765187e-01,  2.67671287e-01,\n",
       "         3.07207614e-01, -1.16485015e-01, -1.61178812e-01,\n",
       "         2.12509945e-01,  3.30767453e-01,  5.39031565e-01,\n",
       "         1.70053974e-01, -7.49765784e-02, -8.21048841e-02,\n",
       "        -1.95452765e-01, -4.37327802e-01, -1.18239023e-01,\n",
       "        -1.72057226e-01, -2.12277025e-01, -3.08992770e-02,\n",
       "         3.28744762e-02, -4.00364324e-02,  3.35174829e-01,\n",
       "        -2.29495183e-01, -6.72506243e-02, -3.56095850e-01,\n",
       "         7.36963093e-01,  2.64157116e-01,  9.18478146e-02,\n",
       "         6.72274679e-02,  4.47071344e-01, -7.57435635e-02,\n",
       "        -2.57053941e-01,  4.44499046e-01,  1.43633962e-01,\n",
       "        -1.54402787e-02,  4.20480102e-01,  3.09378475e-01,\n",
       "        -1.47014245e-01, -2.27981359e-01, -2.75281399e-01,\n",
       "         5.87884597e-02, -7.98739567e-02,  9.27343741e-02,\n",
       "         2.77375489e-01,  6.12378895e-01, -4.06234749e-02,\n",
       "        -6.88882023e-02, -1.80289671e-01,  1.17548481e-01,\n",
       "         6.76128924e-01, -3.30921233e-01, -1.81793928e-01,\n",
       "        -4.79830682e-01, -1.62885308e-01, -7.10228235e-02,\n",
       "        -1.18550129e-01,  2.52150209e-03,  6.43751919e-01,\n",
       "         3.92165691e-01, -3.44340831e-01, -2.67987788e-01,\n",
       "        -1.10531524e-01, -4.82478827e-01, -2.12943344e-03,\n",
       "         2.18253478e-01, -6.68099672e-02,  1.53888147e-02,\n",
       "        -3.59648943e-01,  2.59359740e-02,  2.40339264e-01,\n",
       "         2.15742946e-01,  3.71507078e-01, -1.75250217e-01,\n",
       "         1.69212848e-01,  3.68786275e-01, -1.19587287e-01,\n",
       "         3.29544470e-02, -1.94080040e-01, -1.39329955e-01,\n",
       "        -1.83413818e-01, -1.55592710e-01, -5.80897033e-01,\n",
       "         4.05521318e-02,  1.11588791e-01,  3.28956097e-01,\n",
       "         4.25166219e-01, -1.08033843e-01, -1.66562721e-01,\n",
       "         7.57294744e-02, -2.42595419e-01,  2.71702148e-02,\n",
       "         4.44187909e-01, -1.17501184e-01,  7.89878726e-01,\n",
       "        -4.74001795e-01, -1.65769652e-01, -2.17145234e-01,\n",
       "        -7.53506660e-01,  5.43838739e-01, -1.11963496e-01,\n",
       "         2.91522652e-01,  1.71095505e-01,  6.32001698e-01,\n",
       "        -1.94409832e-01,  2.06371322e-01, -2.05516472e-01,\n",
       "        -1.58781266e+00,  1.56639740e-01,  1.00670673e-01,\n",
       "        -2.21010610e-01,  1.95508450e-01,  2.40227684e-01,\n",
       "         3.77470642e-01, -2.83069964e-02,  2.04945896e-02,\n",
       "        -7.40516186e-03, -1.90225974e-01,  5.94387472e-01,\n",
       "         2.20737070e-01,  5.53308316e-02, -2.09907055e-01,\n",
       "        -4.04061526e-01,  4.58914995e-01, -8.17324758e-01,\n",
       "        -7.83966929e-02, -2.65759099e-02,  4.54465533e-03,\n",
       "        -2.10318100e-02,  4.17821020e-01,  1.47407398e-01,\n",
       "        -3.53109866e-01,  3.65656465e-01,  3.24353039e-01,\n",
       "        -3.76919895e-01, -2.57604986e-01, -1.62864074e-01,\n",
       "        -7.78162837e-01,  4.48547095e-01,  2.47310027e-01,\n",
       "         2.40485638e-01,  3.53251457e-01, -4.24542993e-01,\n",
       "         1.76620007e-01,  2.12511588e-02,  5.68229482e-02,\n",
       "         2.65750557e-01, -6.68548405e-01, -1.91535860e-01,\n",
       "        -7.16939390e-01,  8.42343509e-01, -6.67995930e-01,\n",
       "         9.57336843e-01,  1.67715177e-01,  3.54616851e-01,\n",
       "         5.52576743e-02,  5.67473352e-01, -2.66760355e-03,\n",
       "        -3.94852728e-01,  3.55245657e-02,  6.66239485e-02,\n",
       "         7.89389983e-02,  4.46565717e-01, -4.83335741e-02,\n",
       "         1.81138888e-01, -2.05906510e-01, -3.21594387e-01,\n",
       "        -2.54935265e-01,  2.38172650e-01,  3.59634221e-01,\n",
       "        -4.60769802e-01,  1.51373506e-01, -4.27983820e-01,\n",
       "        -3.41297239e-01, -1.12758055e-01,  5.22110984e-02,\n",
       "        -3.98084074e-01, -7.10423514e-02, -5.64321816e-01,\n",
       "         2.91438606e-02, -5.30487895e-01, -2.48351172e-01,\n",
       "        -3.01186532e-01, -1.98811665e-01, -4.55069125e-01,\n",
       "         2.37282947e-01,  1.00621209e-01,  4.73433197e-01,\n",
       "        -3.89038533e-01,  6.74212635e-01, -8.09904188e-02,\n",
       "        -1.81785434e-01, -1.46380261e-01,  1.56378031e-01,\n",
       "         1.54651038e-03,  5.73404431e-01, -5.99093795e-01,\n",
       "        -4.32134360e-01,  2.22034588e-01, -2.73402244e-01,\n",
       "         4.72311050e-01, -5.36126733e-01, -3.09413761e-01,\n",
       "         1.15547791e-01, -2.81168431e-01,  2.88560335e-02,\n",
       "        -2.91043043e-01,  2.19313219e-01,  4.82949048e-01,\n",
       "        -4.90512550e-01,  1.10717371e-01, -2.87872285e-01,\n",
       "         1.77627757e-01, -2.61318982e-01,  4.06583101e-01,\n",
       "         2.49944657e-01, -4.82517868e-01, -4.18738931e-01,\n",
       "        -1.56457089e-02, -4.50095683e-01, -5.07480741e-01,\n",
       "         3.64913940e-01, -2.65644938e-01, -9.99081656e-02,\n",
       "         1.48642272e-01,  8.60727802e-02, -2.93415338e-01,\n",
       "         2.16729254e-01,  4.85414833e-01, -6.20819628e-01,\n",
       "        -5.01988120e-02, -8.90109718e-01,  2.76869446e-01,\n",
       "        -1.57008886e-01, -2.90923685e-01, -2.03429246e+00,\n",
       "         4.03729051e-01, -3.22706759e-01, -1.29763216e-01,\n",
       "         6.74006999e-01, -3.75498295e-01, -2.48631448e-01,\n",
       "        -1.84526458e-01, -5.64028502e-01,  1.39547959e-01,\n",
       "        -4.86343384e-01, -4.38768178e-01, -7.27138147e-02,\n",
       "         3.94555256e-02,  2.45731488e-01,  3.11012119e-01,\n",
       "        -2.32980996e-01, -6.09175503e-01, -2.38214999e-01,\n",
       "        -6.72771269e-03,  1.11294769e-01, -3.12042655e-03,\n",
       "         2.94508308e-01, -3.86739045e-01,  7.58939326e-01,\n",
       "        -2.90064424e-01, -1.25766128e-01, -2.03103319e-01,\n",
       "        -2.50614733e-01, -2.94494778e-01, -1.15214534e-01,\n",
       "        -4.03106064e-01, -3.12999666e-01,  8.99550021e-01,\n",
       "         9.85100120e-02, -3.14589828e-01, -2.60030568e-01,\n",
       "        -7.32603550e-01, -2.23017827e-01, -1.69183329e-01,\n",
       "        -1.38949588e-01, -7.28525296e-02,  1.74900129e-01,\n",
       "         1.56862631e-01,  6.83028400e-01, -7.86418319e-02,\n",
       "        -3.70275289e-01, -2.59539843e-01, -2.58846432e-01,\n",
       "        -5.95166758e-02,  2.13213041e-01, -9.46476981e-02,\n",
       "         2.26152316e-01, -2.83589363e-02, -3.62259567e-01,\n",
       "         1.40095130e-01,  2.60885537e-01,  2.32210800e-01,\n",
       "        -1.86655596e-01, -1.60275325e-01,  8.88552785e-01,\n",
       "         2.83816934e-01,  2.71940529e-02, -1.58851638e-01,\n",
       "        -4.12444293e-01, -3.78529221e-01, -4.54086632e-01,\n",
       "        -4.26779330e-01, -2.69150317e-01,  8.04910287e-02,\n",
       "        -5.28628170e-01,  5.60974002e-01,  3.82526517e-02,\n",
       "        -5.47146201e-01,  2.83830643e-01,  3.30889337e-02,\n",
       "        -3.30563039e-01, -1.15978576e-01,  1.76741287e-01,\n",
       "         5.32221533e-02, -1.72980383e-01, -1.09192610e-01,\n",
       "        -2.04178795e-01, -2.52933472e-01,  4.82841916e-02,\n",
       "        -2.19572321e-01, -1.69872284e-01, -2.38689810e-01,\n",
       "        -6.71016872e-02,  4.67576906e-02,  1.06004819e-01,\n",
       "        -1.61880821e-01,  6.98564351e-01,  7.96959773e-02,\n",
       "        -2.89673544e-02, -4.83476609e-01,  4.34552729e-01,\n",
       "        -3.80498287e-03,  1.49749771e-01,  1.29555482e-02,\n",
       "         2.91683525e-01, -3.99205498e-02,  3.06826919e-01,\n",
       "        -9.22518596e-02, -1.51983589e-01,  4.82806377e-02,\n",
       "        -6.87938333e-01, -1.75935730e-01,  6.11850694e-02,\n",
       "        -1.30090922e-01,  2.08607644e-01, -3.13328654e-01,\n",
       "         2.25464791e-01, -4.25118625e-01, -4.98898998e-02,\n",
       "        -3.25931042e-01,  4.57198054e-01,  6.71742320e-01,\n",
       "         6.36982262e-01, -3.15562598e-02, -1.19370408e-01,\n",
       "         9.16541755e-01, -2.68358320e-01,  2.10541740e-01,\n",
       "        -7.36255109e-01,  8.19931179e-02,  1.95539042e-01,\n",
       "        -2.93821067e-01, -9.85478982e-02, -2.21591070e-01,\n",
       "        -3.89065772e-01, -3.51394176e-01, -2.71201693e-02,\n",
       "         9.37220380e-02,  2.28577971e-01,  5.71787238e-01,\n",
       "         1.89006068e-02, -2.73248166e-01, -3.71019274e-01,\n",
       "         1.50523022e-01,  2.24794716e-01, -1.27314761e-01,\n",
       "         2.85392076e-01, -4.52010445e-02,  8.02224576e-01,\n",
       "         2.85796463e-01,  7.86167204e-01,  4.92022485e-01,\n",
       "         4.58571576e-02,  4.06791776e-01, -6.25684336e-02,\n",
       "        -2.81980783e-01,  2.26353213e-01,  2.51665413e-01,\n",
       "        -2.05506057e-01, -2.55241781e-01,  4.73848194e-01,\n",
       "         7.16604143e-02, -3.03349078e-01, -8.24046969e-01,\n",
       "         2.74727374e-01,  2.30120704e-01,  5.33841074e-01,\n",
       "         1.70487404e-01,  1.33798039e-02,  1.79617956e-01,\n",
       "         6.14237964e-01,  5.00174463e-02, -1.42202213e-01,\n",
       "         2.88678944e-01, -2.19533056e-01, -9.84631777e-02,\n",
       "         2.22241193e-01,  4.87656184e-02, -2.24169895e-01,\n",
       "         3.14749658e-01, -7.38282427e-02, -6.35275245e-01,\n",
       "        -8.26797038e-02,  3.35724115e-01,  2.59668827e-02,\n",
       "        -2.51003772e-01, -6.61426902e-01,  2.29152739e-02,\n",
       "         2.94691116e-01,  3.58668000e-01, -7.35828802e-02,\n",
       "        -3.00162703e-01,  3.52405816e-01, -2.83000201e-01,\n",
       "        -2.47393027e-01,  3.70847285e-01, -1.85922757e-01,\n",
       "        -3.62273902e-01, -3.14569652e-01, -2.49940604e-01,\n",
       "         5.42105854e-01,  3.94697227e-02,  9.88883376e-02,\n",
       "         4.50009517e-02,  6.18895650e-01,  3.12271416e-01,\n",
       "        -1.74768895e-01, -2.12963179e-01,  2.54709184e-01,\n",
       "         4.06050831e-01, -2.07004502e-01,  1.68601230e-01,\n",
       "        -1.87914073e-02,  6.92082278e-04,  9.92783532e-02,\n",
       "        -8.43716919e-01, -4.35021460e-01, -3.77146274e-01,\n",
       "         6.31927922e-02,  5.38195312e-01,  5.96152479e-03,\n",
       "         2.13880762e-01, -1.31368205e-01,  1.56649742e-02,\n",
       "        -4.30647433e-01, -3.54033224e-02,  4.91531372e-01,\n",
       "         4.96096909e-02, -9.10267889e-01, -4.72567856e-01,\n",
       "        -8.84266421e-02, -8.42720047e-02, -1.03481099e-01,\n",
       "         2.87423998e-01,  6.08843639e-02, -4.01168138e-01,\n",
       "         1.33918881e-01,  2.98966885e-01,  2.75813509e-02,\n",
       "        -2.34644726e-01, -8.71630132e-01,  3.62156600e-01,\n",
       "         9.30158496e-02,  2.06619889e-01, -2.10871384e-01,\n",
       "        -2.74246544e-01, -5.77773213e-01, -2.38017112e-01,\n",
       "         2.78104693e-01, -3.82021010e-01,  1.50336519e-01,\n",
       "         4.99213964e-01, -1.22043658e-02, -1.41802162e-01,\n",
       "         1.31190404e-01,  4.84789237e-02,  9.95351672e-02,\n",
       "        -4.78661448e-01,  2.94228554e-01, -1.98602065e-01,\n",
       "        -5.69054246e-01, -2.27880403e-01, -3.94907653e-01,\n",
       "        -4.81573135e-01, -4.11237150e-01,  3.27318132e-01,\n",
       "         3.35801780e-01, -9.65122581e-02,  1.48486376e-01,\n",
       "         8.88810009e-02,  9.17389523e-03, -8.94951373e-02,\n",
       "         4.12104905e-01, -2.31040910e-01, -5.67629635e-02,\n",
       "         1.90995112e-01,  7.29710907e-02,  4.50867653e-01,\n",
       "         8.08196068e-02, -3.27420533e-01, -1.30515173e-01,\n",
       "        -6.99442029e-01, -1.13703303e-01, -3.84695977e-01,\n",
       "        -5.80909364e-02,  4.94528472e-01,  8.29153061e-02,\n",
       "        -1.72924876e-01, -6.48002103e-02, -3.45220089e-01,\n",
       "        -6.45750528e-03, -1.63017273e-01,  1.60044104e-01,\n",
       "        -1.03079967e-01,  4.72192347e-01,  1.54737160e-01,\n",
       "        -5.57878077e-01,  3.15696508e-01, -2.59207100e-01,\n",
       "         3.33981454e-01,  2.04433754e-01,  4.15317357e-01,\n",
       "         3.08126658e-01,  2.90348798e-01, -2.26124853e-01,\n",
       "         5.29574215e-01,  3.54954004e-01, -1.13061905e-01,\n",
       "        -1.15917161e-01, -3.10784318e-02, -2.73227803e-02,\n",
       "        -7.30421022e-02, -4.59593087e-01, -4.26565222e-02,\n",
       "        -2.29139403e-01, -5.31166673e-01,  4.75830168e-01,\n",
       "         3.81803572e-01, -7.60456383e-01,  3.20510060e-01,\n",
       "         1.81927964e-01, -1.60272926e-01, -2.19470367e-01,\n",
       "        -5.12709543e-02,  5.20137906e-01, -2.21298143e-01,\n",
       "         4.97596949e-01,  3.14845264e-01,  2.98159778e-01,\n",
       "         5.83703101e-01, -2.19947726e-01, -4.77789581e-01,\n",
       "        -1.63761511e-01,  4.89609182e-01,  2.01805197e-02,\n",
       "        -4.63704586e-01,  2.43433803e-01,  8.05759311e-01,\n",
       "        -1.57002285e-01, -2.38256544e-01, -4.17876512e-01,\n",
       "         2.35703945e-01, -3.02357256e-01, -1.33420691e-01,\n",
       "         1.14551291e-01,  3.33907977e-02,  1.36295231e-02,\n",
       "         1.58040956e-01,  6.97139144e-01,  3.22541326e-01,\n",
       "         1.44342750e-01, -5.60420215e-01,  3.30233902e-01,\n",
       "        -2.72676736e-01,  7.94168487e-02,  4.46033210e-01,\n",
       "         2.57624388e-01,  1.62739202e-01, -2.99704313e-01,\n",
       "         4.24976163e-02,  7.31947944e-02,  1.78240314e-01,\n",
       "         4.38307345e-01,  2.49417335e-01, -5.41694701e-01,\n",
       "        -3.43829006e-01, -7.48941079e-02, -3.90162617e-01,\n",
       "        -1.18349358e-01,  3.79168302e-01, -1.13952942e-01,\n",
       "        -2.80082494e-01, -6.02183118e-02,  1.10619672e-01,\n",
       "         4.66228202e-02,  5.14904037e-02, -5.69350608e-02,\n",
       "        -1.99797481e-01, -2.05137804e-01,  2.32846141e-01,\n",
       "         7.22395182e-02, -5.51732898e-01, -1.71852261e-01,\n",
       "        -5.61763309e-02, -2.66830415e-01,  6.63787276e-02,\n",
       "        -1.62210822e-01, -2.17469901e-01,  8.02954100e-03,\n",
       "        -8.66953909e-01, -2.08410740e-01, -3.15012127e-01,\n",
       "        -2.40468346e-02, -4.16769892e-01,  3.58859509e-01,\n",
       "        -2.48973131e-01,  3.68831903e-01, -2.24128753e-01,\n",
       "        -1.05131781e-02, -2.25858524e-01,  4.66621935e-01,\n",
       "         1.33982182e-01,  4.22566503e-01,  1.64091006e-01,\n",
       "         1.09425008e-01, -5.72212756e-01, -3.05888832e-01,\n",
       "         3.41449648e-01,  7.82198533e-02, -5.35649471e-02,\n",
       "         3.56249124e-01,  1.18759431e-01, -2.87536204e-01,\n",
       "         3.55733305e-01,  2.49201670e-01,  7.30968356e-01,\n",
       "        -8.07275891e-01,  1.57730114e-02, -1.43827265e-02,\n",
       "         1.94724992e-01,  2.79638376e-02,  2.47345001e-01,\n",
       "        -9.79539275e-01,  2.19950274e-01,  3.75152640e-02,\n",
       "         9.61078405e-02,  1.72212590e-02,  9.96354446e-02,\n",
       "        -4.07526314e-01,  7.90521428e-02,  5.45622528e-01,\n",
       "         2.93155491e-01, -8.87381658e-02, -3.39239031e-01,\n",
       "        -3.45890597e-02,  1.28177732e-01,  3.10282022e-01,\n",
       "         6.73116148e-01,  1.46816060e-01, -7.35523775e-02,\n",
       "         3.57287191e-02,  2.36172691e-01, -4.48872030e-01,\n",
       "         3.30888540e-01,  2.18155712e-01,  5.39651215e-01,\n",
       "         7.49637410e-02, -1.24985697e-02, -5.16676664e-01,\n",
       "        -5.63841701e-01, -3.93981874e-01, -3.85965794e-01,\n",
       "        -3.67686570e-01, -2.11394280e-01, -1.56570047e-01,\n",
       "         1.22100720e-02, -2.54760444e-01, -6.37801349e-01,\n",
       "         4.90275592e-01, -8.85780901e-02,  1.58695191e-01,\n",
       "        -3.34312707e-01, -2.77821600e-01, -9.06251520e-02]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embeddings('zxy1298')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "- Meta-Llama-3-8B model gave me a score of 0.68\n",
    "- BERT gave a scode of 0.79\n",
    "- Meta-Llama-3-8B model gave me a score of 0.68\n",
    "- Llama-3.1-70B, does not fit in a single GPU, i am testing on CPU, it generates embeddings of 8192 dimenssions, which is higher than 4096 which is the max dimenssion of the vector index of neo4j! just confirm that the mebeddings of 3.1-70B is 8192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "email_handle_embeddings = [get_embeddings(email_handle)[0] for email_handle in emails_handles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct.\n403 Client Error. (Request ID: Root=1-6732df2d-33a592627ed5471a669484c9;92023bf1-9479-4315-a7b2-cd432fbefca4)\n\nCannot access gated repo for url https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct/resolve/main/config.json.\nAccess to model nur-dev/llama-1.9B-kaz-instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    420\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m     )\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-6732df2d-33a592627ed5471a669484c9;92023bf1-9479-4315-a7b2-cd432fbefca4)\n\nCannot access gated repo for url https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct/resolve/main/config.json.\nAccess to model nur-dev/llama-1.9B-kaz-instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct to ask for access.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model and tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnur-dev/llama-1.9B-kaz-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_directory)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(text):\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/transformers/modeling_utils.py:3613\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3612\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 3613\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3623\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3624\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;66;03m# In case one passes a config to `from_pretrained` + \"attn_implementation\"\u001b[39;00m\n\u001b[1;32m   3630\u001b[0m     \u001b[38;5;66;03m# override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3634\u001b[0m     \u001b[38;5;66;03m# we pop attn_implementation from the kwargs but this handles the case where users\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[38;5;66;03m# passes manually the config to `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/transformers/configuration_utils.py:545\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 545\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    547\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/transformers/configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/transformers/configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct.\n403 Client Error. (Request ID: Root=1-6732df2d-33a592627ed5471a669484c9;92023bf1-9479-4315-a7b2-cd432fbefca4)\n\nCannot access gated repo for url https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct/resolve/main/config.json.\nAccess to model nur-dev/llama-1.9B-kaz-instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/nur-dev/llama-1.9B-kaz-instruct to ask for access."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES set to: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "print(f\"CUDA_VISIBLE_DEVICES set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [09:43<00:00, 19.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model from the local directory\n",
    "# model_path = \"/home/dan/my_research/my_local_llama/meta-llama/Meta-Llama-3-8B\"#/original\"  # Path to the directory\n",
    "# model_path = \"/home/dan/my_research/my_local_llama/meta-llama/Meta-Llama-3-8B\"#/original\"  # Path to the directory\n",
    "model_path = \"/home/dan/my_research/my_local_llama/meta-llama/Llama-3.1-70B\"#/original\"  # Path to the directory\n",
    "\n",
    "# Load the tokenizer from the model directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the model from the model directory\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# # Test tokenization (optional)\n",
    "# text = \"This is a test sentence.\"\n",
    "# tokens = tokenizer(text)\n",
    "\n",
    "# print(\"Tokens:\", tokens)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path)#, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "def get_embeddings(text):\n",
    "    # Set pad token if not defined\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # or use add_special_tokens if you want a dedicated pad token\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)#.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "# emb = get_embeddings(text)\n",
    "# emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = kg.query(\"\"\"\n",
    "    MATCH (chunk:Chunk) \n",
    "    WHERE chunk.textEmbedding IS NULL \n",
    "    RETURN chunk.chunkId AS id, chunk.text AS text\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0000950170-23-027948-item1-chunk0000', 'text': '>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0001', 'text': \"•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.\\n\\n\\n•\\nCyber resilience: NetApp unifies monitoring, data protection, security, governance, and compliance for total cyber resilience - with consistency and automation across environments. \\n\\n\\n•\\nContinuous operations: NetApp uses AI-driven automation for continuous optimization to service applications and store stateless and stateful applications at the lowest possible costs.\\n\\n\\n•\\nSustainability: NetApp has industry-leading tools to audit consumption, locate waste, and set guardrails to stop overprovisioning.\\n\\n\\nProduct, Solutions and Services Portfolio\\n \\n\\n\\nNetApp's portfolio of cloud services and storage infrastructure is powered by intelligent data management software. Our operations are organized into two segments: Hybrid Cloud and Public Cloud.\\n\\n\\n \\n\\n\\nHybrid Cloud\\n\\n\\nHybrid Cloud \\noffers a portfolio of storage management and infrastructure solutions that help customers recast their traditional data centers into modern data centers with the power of the cloud. Our hybrid cloud portfolio is designed to operate with public clouds to unlock the potential of hybrid, multi-cloud operations. We offer a broad portfolio of cloud-connected all-flash, hybrid-flash, and object storage systems, powered by intelligent data management software. Hybrid Cloud is composed of software, hardware, and related support, as well as professional and other services.\\n\\n\\nIntelligent data management software\"}\n",
      "{'id': '0000950170-23-027948-item1-chunk0002', 'text': \"Intelligent data management software\\n\\n\\nNetApp ONTAP\\n software is our foundational technology that underpins NetApp's critical storage solutions in the data center and the cloud. ONTAP includes various data management and protection features and capabilities, including automatic ransomware protection against cyber-attacks, built-in data transport features, and storage efficiency capabilities. ONTAP provides the flexibility to design and deploy a storage environment across the broadest range of architectures – from on-premises, hybrid, public, and private clouds. It can be used in NAS, SAN, object environments, and software-defined storage (SDS) situations.\\n\\n\\nData integrity and safety are at the heart of any company’s data center. With NetApp’s extensive software tools and utilities, customers can realize their business continuity goals with time, costs, and personnel savings. With \\nNetApp Snapshot\\n, customers can create and manage point-in-time file system copies with no performance impact and minimal storage consumption. This is important for continuous data protection of information in read-only, static, and immutable form. \\nNetApp SnapCenter Backup Management\\n \\ns\\noftware\\n \\nis designed to deliver high-performance backup and recovery for database and application workloads hosted on ONTAP storage. \\nNetApp SnapMirror Data Replication\\n software can replicate data at high speeds across environments. SnapMirror delivers\\n \\n\\n\\n6\\n\\n\\n\\n\\n\\xa0\\n\\n\\nrobust data management capabilities for virtualization, protecting critical data while providing the flexibility to move data between locations and storage tiers, including cloud service providers. \\nNetApp SnapLock Data Compliance \\nsoftware delivers high-performance disk-based data permanence for HDD and SSD deployments.\"}\n",
      "{'id': '0000950170-23-027948-item1-chunk0003', 'text': 'NetApp Astra (Astra)\\n is a fully managed application-aware data management service built for emerging Kubernetes workloads container infrastructures. Astra allows organizations to protect, recover, and move applications deployed on Kubernetes with no software to download, install, manage, or upgrade.\\n \\n\\n\\nStorage infrastructure\\n \\n\\n\\nNetApp All-Flash FAS (AFF A-Series)\\n is a scale-out platform built for virtualized environments, combining low-latency performance via flash memory (also known as a solid-state storage disk) with best-in-class data management, built-in efficiencies, integrated data protection, multiprotocol support, and nondisruptive operations; cloud and on-premises. AFF A-Series, powered by ONTAP, allows customers to connect to clouds for more data services, data tiering, caching, and disaster recovery. The AFF A-Series has a portfolio of products designed for multiple markets and price/performance considerations, from smaller channel commercial market offerings to large-scale, global enterprises.\\n\\n\\nNetApp QLC-Flash FAS (AFF C-Series) \\nis NetApp’s newest family of storage infrastructure solutions. AFF C-Series arrays are sustainable, scalable, and secure solutions for Tier 1 and Tier 2 applications. AFF C-series provides customers capacity flash performance and affordability, so that customers do not need to make compromises. The AFF C-Series is ideal for transitioning from hybrid/HDD to all-flash storage; running non-latency sensitive VMware database applications and file environments; and providing a solution for secondary storage targets for disaster recovery, backup, and tiering.\\n \\n\\n\\nNetApp Fabric Attached Storage (FAS)\\n series\\n \\nare high-volume, high-capacity data storage devices powered by NetApp ONTAP. NetApp FAS Storage Arrays provide customers with a balance of performance and capacity running either disk drives or hybrid-flash configurations. FAS systems are suitable for secondary storage targets for disaster recovery, backup, and tiering.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0004', 'text': 'NetApp E/EF series\\n is built for dedicated, high-bandwidth applications that need simple, fast SAN storage with enterprise-grade reliability. The E-Series is available as a hybrid-flash platform, while the EF-Series is all-flash. On the SANtricity storage operating system, the E/EF-Series storage appliances are designed for performance-sensitive workloads like real-time analytics, high performance computing, and databases.\\n \\n\\n\\nNetApp StorageGRID\\n is a software-defined object storage solution for large archives, media repositories, and web data stores. Using the industry-standard object APIs like the Amazon Simple Storage Service (S3), the StorageGRID solution, running on the ElementOS data management storage operating system, is provided as a NetApp-branded storage solution and as a software-defined solution on third-party hardware.\\n \\n\\n\\n\\xa0\\n\\n\\nPublic Cloud\\n\\n\\nPublic Cloud\\n offers a portfolio of products delivered primarily as-a-service, including related support. This portfolio includes cloud storage and data services and cloud operations services. Our enterprise-class solutions and services enable customers to control and manage storage in the cloud, consume high-performance storage services for primary workloads, and optimize cloud environments for cost and efficiency. These solutions and services are generally available on the leading public clouds, including Amazon AWS, Microsoft Azure, and Google Cloud Platform.\\n \\n\\n\\nCloud storage, data services, and software'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0005', 'text': 'Cloud storage, data services, and software\\n\\n\\nThe NetApp Cloud Volumes Platform is an integrated collection of cloud storage infrastructure and data services. The platform is anchored by \\nNetApp Cloud Volumes ONTAP\\n, a cloud-based software for customers who wish to manage their own cloud storage infrastructure. It is based on the same ONTAP data management software that underpins our storage infrastructure offerings. Fully managed cloud storage offerings are available natively on Microsoft Azure as \\nAzure NetApp Files\\n, on AWS as \\nAmazon FSx for NetApp ONTAP\\n, and on Google Cloud as \\nNetApp Cloud Volumes Service for Google Cloud.\\n \\n\\n\\nManageability\\n\\n\\nAt the heart of our public cloud storage and data service offerings is \\nNetApp\\n \\nBlueXP\\n. BlueXP is a unified control plane that enables customers to manage their entire data landscape through one single, SaaS-delivered point of control. NetApp BlueXP combines storage and data services via its unified control plane to change how hybrid, multicloud environments are managed, optimized, and controlled. An intuitive interface and powerful automation help decrease resource waste, complexity, and the risk of managing diverse environments. It brings customers operational simplicity in a complex world. Within BlueXP are standard and optional capabilities (services) which allow customers to control their data and operations.\\n\\n\\n7'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0006', 'text': '7\\n\\n\\n\\n\\n\\xa0\\n\\n\\nWith \\nBlueXP Sync\\n service, customers can migrate data to the cloud securely and efficiently. Customers can choose where to deploy primary workloads without re-architecting applications or databases. Customers also get a comprehensive, industry-leading portfolio of storage efficiency capabilities. Inline data compression, deduplication, compaction, and cloud tiering (\\nBlueXP Tiering \\nservice) work together to reduce storage costs and maximize data storage. \\nNetApp Backup \\nservice delivers seamless and cost-effective backup and restore capabilities for protecting and archiving cloud and on-premises data managed by ONTAP. \\nBlueXP\\n \\nCompliance \\nservice provides data discovery, mapping, and classification driven by artificial intelligence algorithms with automated controls and reporting for data privacy regulations such as the General Data Protection Regulation (GDPR), California Consumer Privacy Act (CCPA), and more. Lastly, the \\nBlueXP Cache \\nservice delivers fast and secure access to data for users by caching active data sets to distributed offices globally.\\n \\n\\n\\nCloud operations services\\n\\n\\nNetApp Cloud Insights\\n is an infrastructure monitoring tool that gives organizations visibility into their entire infrastructure. It can monitor, troubleshoot, and optimize costs across all resources, including public clouds and private data centers. Working in conjunction with the BlueXP manageability and control plane services, customers can have deep insights into their data operations.\\n\\n\\nOur \\nSpot by NetApp \\nsuite of products delivers a platform for cloud operations, enabling customers to deploy and operate cloud applications reliably and securely in their choice of the cloud while reducing costs and complexity. Combining machine learning, predictive analytics, and cloud automation, the Spot platform continuously optimizes cloud infrastructure and operations to deliver scalable, reliable, and secure application infrastructure.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0007', 'text': \"Another cloud operations service is \\nInstaclustr\\n, our platform that provides fully managed open-source databases, pipelines, and workflow applications delivered as a service. Instaclustr helps organizations deliver cloud-native applications at scale by operating and supporting their data infrastructure through its SaaS platform for those designing and building around open-source technologies.\\n \\n\\n\\nProfessional and Support Services\\n\\n\\nNetApp and our certified services partners offer a comprehensive portfolio of assessment, design, implementation, migration, and proactive support services to help customers optimize the performance and efficiency of their on-premises and hybrid multicloud storage environments. Our portfolio of offerings include strategic consulting, professional, managed, and support services.\\n \\n\\n\\n•\\nNetApp strategic consulting services provide executive-level, high-touch consulting engagements to help organizations facilitate the alignment of their business and technology goals. Our proven expertise can help organizations define long-term data fabric strategies and operations models to drive IT initiatives for digital transformation.\\n \\n\\n\\n•\\nNetApp Professional Services provide the expertise to mitigate risk and streamline the design, implementation, migration, and integration of NetApp hybrid cloud solutions to realize the business benefits of new technology investments faster. Highly skilled services experts help enable secure, optimized environments that deliver the consistent, high-quality outcomes customers expect from the start.\\n \\n\\n\\n•\\nNetApp Managed Services optimize performance and efficiency in hybrid cloud and on-premises environments. Our NetApp experts use proven methodology and best practices to monitor, administer, operate, and optimize customer environments so their organization's IT staff is free to focus on initiatives to move the business forward.\"}\n",
      "{'id': '0000950170-23-027948-item1-chunk0008', 'text': '•\\nNetApp Keystone is our pay-as-you-grow, storage-as-a-service (STaaS) offering that delivers a seamless hybrid cloud experience for those preferring operating expense consumption models to upfront capital expense or leasing. With a unified management console and monthly bill for both on-premises and cloud data storage services, Keystone lets organizations provision and monitor, and even move storage spend across their hybrid cloud environment for financial and operational flexibility. \\n\\n\\n•\\nNetApp Global Support supplies systems, processes, and people wherever needed to provide continuous operation in complex and critical environments, with an emphasis on proactive and preemptive technology support for operational continuity across the NetApp hybrid cloud. Personalized support options provide actionable intelligence to resolve problems faster, reduce downtime, and optimize performance of the entire NetApp ecosystem.\\n\\n\\nSales, Principal Markets, and Distribution Channels\\n\\n\\nWe market and sell our products and services in numerous countries throughout the world.  Our sales efforts are organized around the evolving needs of our current and targeted customers, and our marketing initiatives reflect this focus. NetApp uses a multichannel distribution strategy. We sell our products, solutions and services to end-user business customers and service providers through a direct sales force and an ecosystem of partners, including the leading cloud providers. Our marketing is focused on building our brand reputation, creating market awareness, communicating customer advantages and generating demand for our sales force and channel partners.\\n\\n\\n8'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0009', 'text': '8\\n\\n\\n\\n\\n\\xa0\\n\\n\\nOur diversified customer base spans industry segments and vertical markets such as energy, financial services, government, technology, internet, life sciences, healthcare services, manufacturing, media, entertainment, animation, video postproduction and telecommunications. NetApp focuses primarily on the enterprise storage and data management, cloud storage and cloud operations markets. We design our products to meet the evolving requirements of a hybrid, multicloud world, driven by digital transformation and cloud initiatives.\\n\\n\\nOur partnerships with the industry’s leading cloud, infrastructure, consulting, application, and reseller partners are created with one goal in mind: the success of our customers. Global enterprises, local businesses, and government installations look to NetApp and our ecosystem of partners to help maximize the business value of their IT and cloud investments.\\n\\n\\nWe work with a wide range of partners for our customers, including technology partners, value-added resellers, system integrators, OEMs, service providers and distributors. During fiscal 2023, sales through our indirect channels represented 78% of our net revenues. Our global partner ecosystem is critical to NetApp’s growth and success. We are continually strengthening existing partnerships and investing in new ones to ensure we are meeting the evolving needs of our customers.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0010', 'text': 'As of April 28, 2023, our worldwide sales and marketing functions consisted of approximately 5,700 managers, sales representatives and technical support personnel. We have offices in approximately 25 countries. Sales to customers Arrow Electronics, Inc. and Tech Data Corporation accounted for 24% and 21% of our net revenues, respectively, in fiscal 2023. Information about sales to and accounts receivables from our major customers, segment disclosures, foreign operations and net sales attributable to our geographic regions is included in Note 15 – Segment, Geographic, and Significant Customer Information of the Notes to Consolidated Financial Statements.\\n\\n\\nSeasonality\\n\\n\\nWe have historically experienced a sequential decline in revenues in the first quarter of our fiscal year, as the sales organization spends time developing new business after higher close rates in the fourth quarter, and because sales to European customers are typically weaker during the summer months. We derive a substantial amount of our revenue in any given quarter from customer orders booked in the same quarter. Customer orders and revenues typically follow intra-quarter seasonality patterns weighted toward the back end of the quarter. If recurring services and cloud revenue continue to increase as a percentage of our total revenues, historical seasonal patterns may become less pronounced.\\n\\n\\nBacklog'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0011', 'text': 'Backlog\\n\\n\\nWe manufacture products based on a combination of specific order requirements and forecasts of our customers’ demand. Orders are generally placed by customers on an as-needed basis. A substantial portion of our products is sold on the basis of standard purchase orders that are cancelable prior to shipment without penalty. In certain circumstances, purchase orders are subject to change with respect to quantity of product or timing of delivery resulting from changes in customer requirements. Our business is characterized by seasonal and intra-quarter variability in demand, as well as short lead times and product delivery schedules. Accordingly, backlog at any given time may not be a meaningful indicator of future revenue.\\n\\n\\nManufacturing and Supply Chain'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0012', 'text': 'Manufacturing and Supply Chain\\n\\n\\nWe have outsourced manufacturing operations to third parties located in Fremont, California; San Jose, California; San Antonio, Texas; Guadalajara, Mexico; Schiphol Airport, The Netherlands; Tiszaujvaros, Hungary; Wuxi, China; Taoyuan City, Taiwan; and Singapore. These operations include materials procurement, commodity management, component engineering, test engineering, manufacturing engineering, product assembly, product assurance, quality control, final test, and global logistics. We rely on a limited number of suppliers for materials, as well as several key subcontractors for the production of certain subassemblies and finished systems. We strive to have multiple suppliers qualified to provide critical components where possible and have our products manufactured in a number of locations to mitigate our supply chain risk. Our strategy has been to develop close relationships with our suppliers, maximizing the exchange of critical information and facilitating the implementation of joint quality programs. We use contract manufacturers for the production of major subassemblies and final system configuration. This manufacturing strategy minimizes capital investments and overhead expenditures while creating flexibility for rapid expansion.\\n\\n\\nWe are certified to the International Organization for Standardization (ISO) 9001:2015 and ISO 14001:2015 certification standards. We have been Tier 2 certified under the U.S. Customs and Border Protection’s (CBP) Customs Trade Partnership Against Terrorism (CTPAT) program since January 2015.\\n\\n\\nResearch and Development'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0013', 'text': 'Research and Development\\n\\n\\nOur research and development team delivers innovation to help customers create an evolved cloud experience. Our R&D structure allows us to align and accelerate the execution of our strategies and roadmaps across product groups. We leverage our talent and shared IP for cloud- and hybrid-cloud solutions to remain agile to changing market conditions. Our R&D priorities are to help\\n \\n\\n\\n9\\n\\n\\n\\n\\n\\xa0\\n\\n\\ncustomers break down silos to simplify management, create consistency, and deliver observability across on premises and multiple cloud environments. We design our products and services from the ground up with cloud connectivity in mind, including tiering, disaster recovery, replication, bursting, and migration.\\n\\n\\nWe conduct research and development activities in various locations throughout the world. Total research and development expenses were $956 million in fiscal 2023, and $881 million in each of fiscal 2022 and fiscal 2021. These costs consist primarily of personnel and related costs incurred to conduct product development activities. Although we develop many of our products internally, we also acquire technology through business combinations or through licensing from third parties when appropriate. We believe that technical leadership is essential to our success, and we expect to continue to commit substantial resources to research and development.\\n\\n\\nCompetition\\n\\n\\nWe operate in an industry in which there are rapid technological advances in hardware, software, and related services offerings. Cloud, digital transformation, and artificial intelligence initiatives are driving changes in customer and solution requirements.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0014', 'text': 'We compete with many companies in the markets we serve. Our hybrid cloud solutions primarily compete with legacy IT and storage vendors. Some offer a broad spectrum of products, solutions and services and others offer a more limited set of storage- and data-management products, solutions or services. Additionally, public cloud providers offer customers storage as an operating expense which competes with more traditional storage offerings that customers acquire through capital expenditures. We both partner with and compete against cloud providers with our public cloud software and services. We rarely see legacy vendors competing in the cloud.\\n\\n\\nWe compete with many companies in the cloud operations marketplace, including new companies (startups) and larger software companies who target developers, operations engineering (DevOps) and financial engineering (FinOps). Some companies have single point solutions that compete with one of our services and others are building platforms. Additionally public cloud providers offer similar services on their own cloud.\\n\\n\\nWe face ongoing product and price competition in all areas of our business, including from both branded- and generic-product competitors.\\n \\n\\n\\nOur current and potential competitors may establish cooperative relationships among themselves or with third parties, including some of our partners. It is possible that new competitors or alliances among competitors might emerge and further increase competitive pressures.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0015', 'text': 'We consider our software innovation, cloud integration, and technology partnerships key to our competitive differentiation. We believe our competitive advantage also includes the nature of the relationships we form with our customers and partners worldwide. We strive to deliver an outstanding experience in every interaction we have with our customers and partners through our product, service, and support offerings, which enables us to provide our customers a full range of expertise before, during and after their purchases.\\n\\n\\nProprietary Rights\\n\\n\\nWe generally rely on patent, copyright, trademark, trade secret and contract laws to establish and maintain our proprietary rights in our technology, products and services. While our intellectual property rights are important to our success, we believe that our business is not materially dependent on any particular patent, trademark, copyright, license or other individual intellectual property right. We have been granted, or own by assignment\\n,\\n well over two thousand U.S. patents, hundreds of pending U.S. patent applications, and many corresponding patents and patent applications in other countries. From time to time, we may make certain intellectual property available under an open source license. Our primary trademarks are NetApp and the NetApp design logo, which are registered trademarks in the U.S. and in many other countries.  In addition, we have trademarks and trademark registrations in the U.S. and other countries covering our various product or service names.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0016', 'text': 'We generally enter into confidentiality agreements with our employees, resellers, distributors, customers, and suppliers. In addition, through various licensing arrangements, we receive certain rights to the intellectual property of others. We expect to maintain current licensing arrangements and to secure additional licensing arrangements in the future, as needed and to the extent available on reasonable terms and conditions, to support continued development and sales of our products and services. Some of these licensing arrangements require or may require royalty payments and other licensing fees. The amount of these payments and fees may depend on various factors, including but not limited to the structure of royalty payments; offsetting considerations, if any; and the degree of use of the licensed technology.\\n\\n\\nThe industry in which we compete is characterized by rapidly changing technology, a large number of patents, and frequent claims and related litigation regarding intellectual property rights, and we may be exposed to various risks related to such claims or legal\\n \\n\\n\\n10\\n\\n\\n\\n\\n\\xa0\\n\\n\\nproceedings. If we are unable to protect our intellectual property, we may be subject to increased competition that could materially and adversely affect our business operations, financial condition, results of operations and/or cash flows.\\n\\n\\nEnvironmental Disclosure\\n\\n\\nWe are committed to the success of our customers and partners, to delivering value to our stockholders, and to positively affecting the communities where our employees work and live. We firmly believe that we can accomplish these objectives concurrently with our commitment to sound environmental management. We are committed to the reduction of greenhouse gas emissions; efficient use of natural resources; and minimizing, relative to the growth of the Company, the environmental impacts from our operations, products, and services, as well as complying with laws and regulations related to these areas.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0017', 'text': 'We voluntarily measure, monitor, and publicly report our scope 1, scope 2, and scope 3 (partial) greenhouse gas emissions and water impacts to CDP, a global standardized mechanism by which companies report their greenhouse gas emissions and water impacts to customers and institutional investors. We continuously seek to optimize the energy efficiency of our buildings, labs, and data centers; and we have increased our use of renewable energy, especially at our facilities in Bangalore, India and Wichita, Kansas, both of which are powered almost exclusively by renewable energy.\\n\\n\\nAt both the global and regional/state levels, various laws and regulations have been implemented or are under consideration to mitigate or report on the effects of climate change. Environmental laws are complex, change frequently, and have tended to become more stringent over time. However, it is often difficult to anticipate future regulations pertaining to environmental matters and to estimate their impacts on our operations. Based on current information, we believe that our primary risk related to climate change is the risk of increased energy costs. We are not currently subject to a cap-and-trade system or any other mitigation measures that could be material to our operations, nor are we aware of any such measures that will impact us in the near future. Additionally, we have implemented disaster recovery and business resiliency measures to mitigate the physical risks our facilities, business, and supply chain might face as a consequence of severe weather/climate-related phenomena such as earthquakes, floods, droughts, and other such natural occurrences.'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0018', 'text': 'We are subject to international, federal, state, and local regulations regarding workplace safety and protection of the environment. Various international, federal, state, and local provisions regulate the use and discharge of certain hazardous materials used in the manufacture of our products. Failure to comply with environmental regulations in the future could cause us to incur substantial costs, subject us to business interruptions or cause customers to cease purchasing from us. We strive to comply with all applicable environmental laws. All of our products meet the applicable requirements of the Registration, Evaluation, Authorisation and Restriction of Chemicals (REACH); Energy Related Products (ErP); Restriction of Hazardous Substances (RoHS); and China RoHS directives. We have a product take-back program and an e-waste scheme to comply with the EU directive on Waste Electrical and Electronic Equipment (WEEE), and Extended Producer Responsibility (EPR) regulations in India.\\n\\n\\nWe have maintained an environmental management system since December 2004 that provides the framework for setting, monitoring, and continuously improving our environmental goals and objectives. As part of ISO 14001 requirements, we set local environmental performance goals, such as reducing energy use per square foot and minimizing waste generated on site, that are aligned with our overall corporate strategy. We also conduct periodic reviews and are subject to third-party audits of our operations, and we monitor environmental legislation and requirements to help make sure we are taking necessary measures to remain in compliance with applicable laws, not only in our operations but also for our products.\\n\\n\\nHuman Capital'}\n",
      "{'id': '0000950170-23-027948-item1-chunk0019', 'text': 'Human Capital\\n\\n\\nWe take pride in, and believe our success depends on, attracting and retaining leading talent in the industry based on a culture-fit approach. From our inception, NetApp has worked to build a model company and has embraced a culture of openness and trust. Our employees are supported and encouraged to be innovative, and we communicate openly and transparently so that employees can focus on critical and impactful work that ties directly to our business strategy. We continue to invest in our global workforce to support diversity and inclusion and to support our employees’ well-being and development.\\n \\n\\n\\nDiversity, Inclusion and Belonging\\n\\n\\nWe believe diversity, inclusion and belonging leads to more innovation, better access to talent and improved business outcomes. Our strategies are intended to increase the demographic and cognitive diversity of our employee population, promote a culture of inclusion and to leverage such diversity to achieve business results. For more information about our commitment to diversity, inclusion and belonging, go to the “Diversity Inclusion and Belonging” section of our website.\\n\\n\\nBenefits, Wellbeing and Engagement\\n\\n\\nOur healthcare options offer competitive, comprehensive coverage for our employees and their families, including:\\n \\n\\n\\n11\\n\\n\\n\\n\\n\\xa0\\n\\n\\n•\\nNational medical plans,\\n\\n\\n•\\nRegional medical plans,\\n\\n\\n•\\nExpert advice from world-renowned doctors through our medical second opinion program,\\n\\n\\n•\\nNational dental plans,\\n\\n\\n•\\nNational vision plans with two levels of coverage to choose from and a\\n\\n\\n•\\nRobust wellness program.\\n\\n\\nInsurance and income protection\\n. We provide life, accidental death and dismemberment and disability insurance programs. For additional peace of mind, we also offer supplemental insurance for our employees and their dependents.'}\n",
      "{'id': '0000950170-23-027948-item1a-chunk0000', 'text': '>Item 1A\\n\\n\\n\\xa0\\n\\n\\nRisk Factors\\n\\n\\n\\xa0\\n\\n\\n14'}\n",
      "{'id': '0000950170-23-027948-item7-chunk0000', 'text': '>Item 7\\n\\n\\n\\xa0\\n\\n\\nManagement’s Discussion and Analysis of Financial Condition and Results of Operations\\n\\n\\n\\xa0\\n\\n\\n33'}\n",
      "{'id': '0000950170-23-027948-item7a-chunk0000', 'text': '>Item 7A\\n\\n\\n\\xa0\\n\\n\\nQuantitative and Qualitative Disclosures About Market Risk\\n\\n\\n\\xa0\\n\\n\\n50'}\n"
     ]
    }
   ],
   "source": [
    "for c in chunks:\n",
    "    print (c,)# c['text'], c['id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute embeddings locally\n",
    "chunk_embeddings = {}\n",
    "for chunk in chunks:\n",
    "    text = chunk['text']\n",
    "    chunk_id = chunk['id']\n",
    "    embedding = get_embeddings(text)\n",
    "    chunk_embeddings[chunk_id] = embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save chunk_embeddings to a file\n",
    "with open('chunk_embeddings_Llama_3_1_70B.pkl', 'wb') as file:\n",
    "    pickle.dump(chunk_embeddings, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0000950170-23-027948-item1-chunk0000': array([[-0.5166815 , -0.08864088,  0.19969292, ...,  1.0027709 ,\n",
       "         -1.2434479 , -0.5435967 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0001': array([[-0.20656812, -0.72979623,  0.4156664 , ...,  0.85177326,\n",
       "         -1.8440838 , -0.9053261 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0002': array([[-0.6879011, -0.9348478,  0.5717283, ...,  1.1459522, -2.0685847,\n",
       "         -0.8164457]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0003': array([[-0.45972177,  0.00981281,  0.57732713, ...,  0.7307635 ,\n",
       "         -3.1174972 , -0.4132972 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0004': array([[-0.07823444, -0.17354007, -0.21544048, ...,  0.8408205 ,\n",
       "         -2.853965  , -0.85123557]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0005': array([[-0.21783504, -0.22896345, -0.07046735, ...,  1.181591  ,\n",
       "         -2.3846467 , -0.5290723 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0006': array([[-0.675373  , -0.43458822,  0.10996523, ...,  0.9066282 ,\n",
       "         -2.0865972 , -0.8138677 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0007': array([[-0.2789847 , -0.47081935,  0.5720006 , ...,  0.629902  ,\n",
       "         -2.1456563 , -0.8630543 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0008': array([[ 0.10190742, -0.84657973,  0.23955822, ...,  0.6111157 ,\n",
       "         -2.5707736 , -0.24455085]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0009': array([[-0.8141059 , -0.72460586,  0.19585687, ...,  1.1007742 ,\n",
       "         -1.5753387 , -1.0690852 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0010': array([[ 0.01127671, -1.5108298 ,  0.2542505 , ...,  0.16237167,\n",
       "         -1.3124595 , -0.252947  ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0011': array([[ 0.17153311, -1.1393065 ,  0.39463174, ...,  0.37270045,\n",
       "         -0.8113588 , -0.74380076]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0012': array([[ 0.07360474,  0.03937561,  0.18661894, ..., -0.2658073 ,\n",
       "         -2.0255556 ,  0.40073603]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0013': array([[-0.7596028 , -0.58862996,  0.43569118, ...,  1.254382  ,\n",
       "         -1.5849744 ,  0.03244859]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0014': array([[-0.20963077, -0.8028041 ,  0.5198141 , ...,  1.5641733 ,\n",
       "         -2.6030726 , -0.25977397]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0015': array([[ 0.40611285, -0.33177668, -0.6502192 , ...,  1.335838  ,\n",
       "         -0.7899427 , -0.7881411 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0016': array([[-0.02894705, -0.00346647, -0.16341248, ...,  1.2664284 ,\n",
       "         -0.29665676, -0.7124925 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0017': array([[ 0.93731636, -0.653209  , -0.2101848 , ...,  0.73126143,\n",
       "         -1.6790546 ,  0.5713267 ]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0018': array([[ 1.2425708, -0.0340189, -0.8289465, ..., -0.0263488, -1.7592514,\n",
       "          0.7838749]], dtype=float32),\n",
       " '0000950170-23-027948-item1-chunk0019': array([[-1.5021718 , -0.15404002,  0.19596638, ...,  1.5069412 ,\n",
       "          0.54252046,  0.27398846]], dtype=float32),\n",
       " '0000950170-23-027948-item1a-chunk0000': array([[ 0.5382168 , -0.08723179, -0.9733321 , ..., -0.09028076,\n",
       "         -0.99772364, -0.29723567]], dtype=float32),\n",
       " '0000950170-23-027948-item7-chunk0000': array([[-0.6021621 , -1.1101049 , -1.1854755 , ...,  0.2744117 ,\n",
       "          0.05552308, -0.16257417]], dtype=float32),\n",
       " '0000950170-23-027948-item7a-chunk0000': array([[ 0.19496787, -1.0380964 , -0.74146783, ..., -0.18041678,\n",
       "         -0.35464078,  0.11824027]], dtype=float32)}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load chunk_embeddings from the file\n",
    "with open('chunk_embeddings_Llama_3_1_70B.pkl', 'rb') as file:\n",
    "    chunk_embeddings_new = pickle.load(file)\n",
    "chunk_embeddings_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8192)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_embeddings['0000950170-23-027948-item1-chunk0000'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5166815 , -0.08864088,  0.19969292, ...,  1.0027709 ,\n",
       "        -1.2434479 , -0.5435967 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_embeddings['0000950170-23-027948-item1-chunk0000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 3: Update Neo4j with computed embeddings\n",
    "for chunk_id, embedding in chunk_embeddings.items():\n",
    "    # Flatten the embedding if it's multi-dimensional\n",
    "    flat_embedding = np.array(embedding).flatten().tolist()  # Ensure it's a 1D list\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "        MATCH (chunk:Chunk {chunkId: $chunk_id})  \n",
    "        SET chunk.textEmbedding = $embedding  \n",
    "    \"\"\", params={\"chunk_id\": chunk_id, \"embedding\": flat_embedding})  # Pass chunk_id and flat_embedding as parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Chunk {chunkId: STRING, names: LIST, formId: STRING, cik: STRING, cusip6: STRING, source: STRING, f10kItem: STRING, chunkSeqId: INTEGER, text: STRING, textEmbedding: LIST}\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg.refresh_schema()\n",
    "print(kg.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use similarity search to find relevant chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here,\n",
    "\n",
    "ok so far i could get llama3 from my local to calculate the emneddings and replace it with openai embeddings, i updated the kg database based on these embeddings, next i need to perform the search using similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  vector_search_query = \"\"\"\n",
    "    WITH genai.vector.encode(\n",
    "      $question, \n",
    "      \"OpenAI\", \n",
    "      {\n",
    "        token: $openAiApiKey,\n",
    "        endpoint: $openAiEndpoint\n",
    "      }) AS question_embedding\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, question_embedding) yield node, score\n",
    "    RETURN score, node.text AS text\n",
    "  \"\"\"\n",
    "  similar = kg.query(vector_search_query, \n",
    "                     params={\n",
    "                      'question': question, \n",
    "                      'openAiApiKey':OPENAI_API_KEY,\n",
    "                      'openAiEndpoint': OPENAI_ENDPOINT,\n",
    "                      'index_name':VECTOR_INDEX_NAME, \n",
    "                      'top_k': 10})\n",
    "  return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question):\n",
    "    \"\"\"Search for similar nodes using the Neo4j vector index with local Llama 3 embeddings.\"\"\"\n",
    "    # Generate the embedding using the local Llama 3 model\n",
    "    question_embedding = get_embeddings(question)\n",
    "    question_embedding = question_embedding.flatten()  # or question_embedding.squeeze()\n",
    "    # question_embedding = question_embedding / np.linalg.norm(question_embedding)  # Normalize if using cosine\n",
    "\n",
    "    # Neo4j query without OpenAI API\n",
    "    vector_search_query = \"\"\"\n",
    "      CALL db.index.vector.queryNodes($index_name, $top_k, $question_embedding) \n",
    "      YIELD node, score\n",
    "      RETURN score, node.text AS text\n",
    "    \"\"\"\n",
    "    \n",
    "    similar = kg.query(vector_search_query, \n",
    "                       params={\n",
    "                           'question_embedding': question_embedding,\n",
    "                           'index_name': VECTOR_INDEX_NAME, \n",
    "                           'top_k': 10\n",
    "                       })\n",
    "    \n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.IllegalArgumentException: Index query vector has 8192 dimensions, but indexed vectors have 11.}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mneo4j_vector_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIn a single sentence, tell me about Netapp.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 15\u001b[0m, in \u001b[0;36mneo4j_vector_search\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# question_embedding = question_embedding / np.linalg.norm(question_embedding)  # Normalize if using cosine\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Neo4j query without OpenAI API\u001b[39;00m\n\u001b[1;32m      9\u001b[0m vector_search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  CALL db.index.vector.queryNodes($index_name, $top_k, $question_embedding) \u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  YIELD node, score\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  RETURN score, node.text AS text\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m similar \u001b[38;5;241m=\u001b[39m \u001b[43mkg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_search_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion_embedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mVECTOR_INDEX_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m similar\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/langchain_community/graphs/neo4j_graph.py:431\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneo4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Neo4jError\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     data, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/driver.py:971\u001b[0m, in \u001b[0;36mDriver.execute_query\u001b[0;34m(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid routing control value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrouting_\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    969\u001b[0m     )\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_pipelined_begin:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDRIVER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_transformer_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/session.py:581\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransaction_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# if cancellation callback has not been called yet:\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_work/query.py:144\u001b[0m, in \u001b[0;36munit_of_work.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/driver.py:1308\u001b[0m, in \u001b[0;36m_work\u001b[0;34m(tx, query, parameters, transformer)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_work\u001b[39m(\n\u001b[1;32m   1302\u001b[0m     tx: ManagedTransaction,\n\u001b[1;32m   1303\u001b[0m     query: te\u001b[38;5;241m.\u001b[39mLiteralString,\n\u001b[1;32m   1304\u001b[0m     parameters: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny],\n\u001b[1;32m   1305\u001b[0m     transformer: t\u001b[38;5;241m.\u001b[39mCallable[[Result], t\u001b[38;5;241m.\u001b[39mUnion[_T]],\n\u001b[1;32m   1306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m   1307\u001b[0m     res \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mrun(query, parameters)\n\u001b[0;32m-> 1308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/result.py:797\u001b[0m, in \u001b[0;36mResult.to_eager_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m\u001b[38;5;241m.\u001b[39m_non_concurrent_method\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_eager_result\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EagerResult:\n\u001b[1;32m    781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    Convert this result to an :class:`.EagerResult`.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 5.8 Stabilized from experimental.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EagerResult(\n\u001b[1;32m    799\u001b[0m         keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m    800\u001b[0m         records\u001b[38;5;241m=\u001b[39mUtil\u001b[38;5;241m.\u001b[39mlist(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    801\u001b[0m         summary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsume(),\n\u001b[1;32m    802\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/result.py:454\u001b[0m, in \u001b[0;36mResult._buffer_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_all\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/result.py:443\u001b[0m, in \u001b[0;36mResult._buffer\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m record_buffer \u001b[38;5;241m=\u001b[39m deque()\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    444\u001b[0m     record_buffer\u001b[38;5;241m.\u001b[39mappend(record)\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(record_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/work/result.py:393\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_common.py:184\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_bolt.py:994\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    991\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    992\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    993\u001b[0m )\n\u001b[0;32m--> 994\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_bolt5.py:496\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[0;32m~/anaconda3/envs/kg-with-local-llama/lib/python3.10/site-packages/neo4j/_sync/io/_common.py:254\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    252\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hydrate_error(metadata)\n",
      "\u001b[0;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.IllegalArgumentException: Index query vector has 8192 dimensions, but indexed vectors have 11.}"
     ]
    }
   ],
   "source": [
    "search_results = neo4j_vector_search(\n",
    "    'In a single sentence, tell me about Netapp.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6802825927734375,\n",
       "  'text': '>Item 7A\\n\\n\\n\\xa0\\n\\n\\nQuantitative and Qualitative Disclosures About Market Risk\\n\\n\\n\\xa0\\n\\n\\n50'},\n",
       " {'score': 0.6581363677978516,\n",
       "  'text': '8\\n\\n\\n\\n\\n\\xa0\\n\\n\\nOur diversified customer base spans industry segments and vertical markets such as energy, financial services, government, technology, internet, life sciences, healthcare services, manufacturing, media, entertainment, animation, video postproduction and telecommunications. NetApp focuses primarily on the enterprise storage and data management, cloud storage and cloud operations markets. We design our products to meet the evolving requirements of a hybrid, multicloud world, driven by digital transformation and cloud initiatives.\\n\\n\\nOur partnerships with the industry’s leading cloud, infrastructure, consulting, application, and reseller partners are created with one goal in mind: the success of our customers. Global enterprises, local businesses, and government installations look to NetApp and our ecosystem of partners to help maximize the business value of their IT and cloud investments.\\n\\n\\nWe work with a wide range of partners for our customers, including technology partners, value-added resellers, system integrators, OEMs, service providers and distributors. During fiscal 2023, sales through our indirect channels represented 78% of our net revenues. Our global partner ecosystem is critical to NetApp’s growth and success. We are continually strengthening existing partnerships and investing in new ones to ensure we are meeting the evolving needs of our customers.'},\n",
       " {'score': 0.6568870544433594,\n",
       "  'text': '•\\nNetApp Keystone is our pay-as-you-grow, storage-as-a-service (STaaS) offering that delivers a seamless hybrid cloud experience for those preferring operating expense consumption models to upfront capital expense or leasing. With a unified management console and monthly bill for both on-premises and cloud data storage services, Keystone lets organizations provision and monitor, and even move storage spend across their hybrid cloud environment for financial and operational flexibility. \\n\\n\\n•\\nNetApp Global Support supplies systems, processes, and people wherever needed to provide continuous operation in complex and critical environments, with an emphasis on proactive and preemptive technology support for operational continuity across the NetApp hybrid cloud. Personalized support options provide actionable intelligence to resolve problems faster, reduce downtime, and optimize performance of the entire NetApp ecosystem.\\n\\n\\nSales, Principal Markets, and Distribution Channels\\n\\n\\nWe market and sell our products and services in numerous countries throughout the world.  Our sales efforts are organized around the evolving needs of our current and targeted customers, and our marketing initiatives reflect this focus. NetApp uses a multichannel distribution strategy. We sell our products, solutions and services to end-user business customers and service providers through a direct sales force and an ecosystem of partners, including the leading cloud providers. Our marketing is focused on building our brand reputation, creating market awareness, communicating customer advantages and generating demand for our sales force and channel partners.\\n\\n\\n8'},\n",
       " {'score': 0.6560459136962891,\n",
       "  'text': 'We consider our software innovation, cloud integration, and technology partnerships key to our competitive differentiation. We believe our competitive advantage also includes the nature of the relationships we form with our customers and partners worldwide. We strive to deliver an outstanding experience in every interaction we have with our customers and partners through our product, service, and support offerings, which enables us to provide our customers a full range of expertise before, during and after their purchases.\\n\\n\\nProprietary Rights\\n\\n\\nWe generally rely on patent, copyright, trademark, trade secret and contract laws to establish and maintain our proprietary rights in our technology, products and services. While our intellectual property rights are important to our success, we believe that our business is not materially dependent on any particular patent, trademark, copyright, license or other individual intellectual property right. We have been granted, or own by assignment\\n,\\n well over two thousand U.S. patents, hundreds of pending U.S. patent applications, and many corresponding patents and patent applications in other countries. From time to time, we may make certain intellectual property available under an open source license. Our primary trademarks are NetApp and the NetApp design logo, which are registered trademarks in the U.S. and in many other countries.  In addition, we have trademarks and trademark registrations in the U.S. and other countries covering our various product or service names.'},\n",
       " {'score': 0.6507434844970703,\n",
       "  'text': 'We compete with many companies in the markets we serve. Our hybrid cloud solutions primarily compete with legacy IT and storage vendors. Some offer a broad spectrum of products, solutions and services and others offer a more limited set of storage- and data-management products, solutions or services. Additionally, public cloud providers offer customers storage as an operating expense which competes with more traditional storage offerings that customers acquire through capital expenditures. We both partner with and compete against cloud providers with our public cloud software and services. We rarely see legacy vendors competing in the cloud.\\n\\n\\nWe compete with many companies in the cloud operations marketplace, including new companies (startups) and larger software companies who target developers, operations engineering (DevOps) and financial engineering (FinOps). Some companies have single point solutions that compete with one of our services and others are building platforms. Additionally public cloud providers offer similar services on their own cloud.\\n\\n\\nWe face ongoing product and price competition in all areas of our business, including from both branded- and generic-product competitors.\\n \\n\\n\\nOur current and potential competitors may establish cooperative relationships among themselves or with third parties, including some of our partners. It is possible that new competitors or alliances among competitors might emerge and further increase competitive pressures.'},\n",
       " {'score': 0.649022102355957,\n",
       "  'text': 'Human Capital\\n\\n\\nWe take pride in, and believe our success depends on, attracting and retaining leading talent in the industry based on a culture-fit approach. From our inception, NetApp has worked to build a model company and has embraced a culture of openness and trust. Our employees are supported and encouraged to be innovative, and we communicate openly and transparently so that employees can focus on critical and impactful work that ties directly to our business strategy. We continue to invest in our global workforce to support diversity and inclusion and to support our employees’ well-being and development.\\n \\n\\n\\nDiversity, Inclusion and Belonging\\n\\n\\nWe believe diversity, inclusion and belonging leads to more innovation, better access to talent and improved business outcomes. Our strategies are intended to increase the demographic and cognitive diversity of our employee population, promote a culture of inclusion and to leverage such diversity to achieve business results. For more information about our commitment to diversity, inclusion and belonging, go to the “Diversity Inclusion and Belonging” section of our website.\\n\\n\\nBenefits, Wellbeing and Engagement\\n\\n\\nOur healthcare options offer competitive, comprehensive coverage for our employees and their families, including:\\n \\n\\n\\n11\\n\\n\\n\\n\\n\\xa0\\n\\n\\n•\\nNational medical plans,\\n\\n\\n•\\nRegional medical plans,\\n\\n\\n•\\nExpert advice from world-renowned doctors through our medical second opinion program,\\n\\n\\n•\\nNational dental plans,\\n\\n\\n•\\nNational vision plans with two levels of coverage to choose from and a\\n\\n\\n•\\nRobust wellness program.\\n\\n\\nInsurance and income protection\\n. We provide life, accidental death and dismemberment and disability insurance programs. For additional peace of mind, we also offer supplemental insurance for our employees and their dependents.'},\n",
       " {'score': 0.6476287841796875,\n",
       "  'text': '>Item 7\\n\\n\\n\\xa0\\n\\n\\nManagement’s Discussion and Analysis of Financial Condition and Results of Operations\\n\\n\\n\\xa0\\n\\n\\n33'},\n",
       " {'score': 0.6475553512573242,\n",
       "  'text': \"•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.\\n\\n\\n•\\nCyber resilience: NetApp unifies monitoring, data protection, security, governance, and compliance for total cyber resilience - with consistency and automation across environments. \\n\\n\\n•\\nContinuous operations: NetApp uses AI-driven automation for continuous optimization to service applications and store stateless and stateful applications at the lowest possible costs.\\n\\n\\n•\\nSustainability: NetApp has industry-leading tools to audit consumption, locate waste, and set guardrails to stop overprovisioning.\\n\\n\\nProduct, Solutions and Services Portfolio\\n \\n\\n\\nNetApp's portfolio of cloud services and storage infrastructure is powered by intelligent data management software. Our operations are organized into two segments: Hybrid Cloud and Public Cloud.\\n\\n\\n \\n\\n\\nHybrid Cloud\\n\\n\\nHybrid Cloud \\noffers a portfolio of storage management and infrastructure solutions that help customers recast their traditional data centers into modern data centers with the power of the cloud. Our hybrid cloud portfolio is designed to operate with public clouds to unlock the potential of hybrid, multi-cloud operations. We offer a broad portfolio of cloud-connected all-flash, hybrid-flash, and object storage systems, powered by intelligent data management software. Hybrid Cloud is composed of software, hardware, and related support, as well as professional and other services.\\n\\n\\nIntelligent data management software\"},\n",
       " {'score': 0.6426820755004883,\n",
       "  'text': 'Research and Development\\n\\n\\nOur research and development team delivers innovation to help customers create an evolved cloud experience. Our R&D structure allows us to align and accelerate the execution of our strategies and roadmaps across product groups. We leverage our talent and shared IP for cloud- and hybrid-cloud solutions to remain agile to changing market conditions. Our R&D priorities are to help\\n \\n\\n\\n9\\n\\n\\n\\n\\n\\xa0\\n\\n\\ncustomers break down silos to simplify management, create consistency, and deliver observability across on premises and multiple cloud environments. We design our products and services from the ground up with cloud connectivity in mind, including tiering, disaster recovery, replication, bursting, and migration.\\n\\n\\nWe conduct research and development activities in various locations throughout the world. Total research and development expenses were $956 million in fiscal 2023, and $881 million in each of fiscal 2022 and fiscal 2021. These costs consist primarily of personnel and related costs incurred to conduct product development activities. Although we develop many of our products internally, we also acquire technology through business combinations or through licensing from third parties when appropriate. We believe that technical leadership is essential to our success, and we expect to continue to commit substantial resources to research and development.\\n\\n\\nCompetition\\n\\n\\nWe operate in an industry in which there are rapid technological advances in hardware, software, and related services offerings. Cloud, digital transformation, and artificial intelligence initiatives are driving changes in customer and solution requirements.'},\n",
       " {'score': 0.6421003341674805,\n",
       "  'text': 'NetApp E/EF series\\n is built for dedicated, high-bandwidth applications that need simple, fast SAN storage with enterprise-grade reliability. The E-Series is available as a hybrid-flash platform, while the EF-Series is all-flash. On the SANtricity storage operating system, the E/EF-Series storage appliances are designed for performance-sensitive workloads like real-time analytics, high performance computing, and databases.\\n \\n\\n\\nNetApp StorageGRID\\n is a software-defined object storage solution for large archives, media repositories, and web data stores. Using the industry-standard object APIs like the Amazon Simple Storage Service (S3), the StorageGRID solution, running on the ElementOS data management storage operating system, is provided as a NetApp-branded storage solution and as a software-defined solution on third-party hardware.\\n \\n\\n\\n\\xa0\\n\\n\\nPublic Cloud\\n\\n\\nPublic Cloud\\n offers a portfolio of products delivered primarily as-a-service, including related support. This portfolio includes cloud storage and data services and cloud operations services. Our enterprise-class solutions and services enable customers to control and manage storage in the cloud, consume high-performance storage services for primary workloads, and optimize cloud environments for cost and efficiency. These solutions and services are generally available on the leading public clouds, including Amazon AWS, Microsoft Azure, and Google Cloud Platform.\\n \\n\\n\\nCloud storage, data services, and software'}]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "question_embedding = get_embeddings('In a single sentence, tell me about Netapp.')\n",
    "print(question_embedding.flatten().shape)  # Should output (4096,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nodes': [{'name': 'Chunk', 'indexes': ['textEmbedding'], 'constraints': [\"Constraint( id=3, name='unique_chunk', type='UNIQUENESS', schema=(:Chunk {chunkId}), ownedIndex=2 )\"]}], 'relationships': []}\n"
     ]
    }
   ],
   "source": [
    "# Assuming `kg` is your Neo4j driver session or connection\n",
    "indexes = kg.query(\"CALL db.schema.visualization\")\n",
    "\n",
    "# Display the list of schema details, including indexes\n",
    "for index in indexes:\n",
    "    print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "# Assuming `kg` is your Neo4j connection\n",
    "query = \"MATCH (c:Chunk) RETURN c.textEmbedding LIMIT 1\"\n",
    "result = kg.query(query)\n",
    "\n",
    "# Assuming the result is a list of dictionaries with the 'textEmbedding' key\n",
    "embedding = result[0]['c.textEmbedding']\n",
    "\n",
    "# Check the length of the embedding to see its dimensionality\n",
    "print(len(embedding))  # This will print the number of dimensions in the embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg-with-local-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
